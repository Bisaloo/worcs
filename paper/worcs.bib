
@article{aalbersbergMakingScienceTransparent2018,
  title = {Making {{Science Transparent By Default}}; {{Introducing}} the {{TOP Statement}}},
  author = {Aalbersberg, IJsbrand Jan and Appleyard, Tom and Brookhart, Sarah and Carpenter, Todd and Clarke, Michael and Curry, Stephen and Dahl, Josh and DeHaven, Alexander Carl and Eich, Eric and Franko, Maryrose and Freedman, Len and Graf, Chris and Grant, Sean and Hanson, Brooks and Joseph, Heather and Kiermer, Veronique and Kramer, Bianca and Kraut, Alan and Karn, Roshan Kumar and Lee, Carole and MacFarlane, Aki and Martone, Maryann and Mayo-Wilson, Evan and McNutt, Marcia and McPhail, Meredith and Mellor, David Thomas and Moher, David and Mudditt, Alison and Nosek, Brian A. and Orland, Belinda and Parker, Timothy H. and Parsons, Mark and Patterson, Mark and Santos, Solange and Shore, Carolyn and Simons, Daniel J. and Spellman, Bobbie and Spies, Jeffrey Robert and Spitzer, Matthew and Stodden, Victoria and Swaminathan, Sowmya and Sweet, Deborah and Tsui, Anne and Vazire, Simine},
  date = {2018-02-15},
  doi = {10.31219/osf.io/sm78t},
  url = {https://osf.io/sm78t},
  urldate = {2020-01-08},
  abstract = {In order to increase the replicability of scientific work, the scientific community has called for practices designed to increase the transparency of research (McNutt, 2014; Nosek et al., 2015). The validity of a scientific claim depends not on the reputation of those making the claim, the venue in which the claim is made, or the novelty of the result, but rather on the empirical evidence provided by the underlying data and methods. Proper evaluation of  the merits of scientific findings requires availability of the methods, materials, and data and the reasoned argument that serve as the basis for the published conclusions (Claerbout and Karrenbach 1992; Donoho et al 2009; Stodden et al 2013; Borwein et al 2013; Munafò et al, 2017). Wide and growing support for these principles (see, for example, signatories to Declaration on Research Assessment, DORA, https://sfdora.org/, and the Transparency and Openness Promotion Guidelines https://cos.io/our-services/top-guidelines/) must be coupled with guidelines to increase open sharing of data and research materials, use of reporting guidelines, preregistration, and replication. We propose that, going forward, authors of all scientific articles disclose the availability and location of all research items, including data, materials, and code, related to their published articles in what we will refer to as a TOP Statement.}
}

@article{aczelConsensusbasedTransparencyChecklist2019,
  title = {A Consensus-Based Transparency Checklist},
  author = {Aczel, Balazs and Szaszi, Barnabas and Sarafoglou, Alexandra and Kekecs, Zoltan and Kucharský, Šimon and Benjamin, Daniel and Chambers, Christopher D. and Fisher, Agneta and Gelman, Andrew and Gernsbacher, Morton A. and Ioannidis, John P. and Johnson, Eric and Jonas, Kai and Kousta, Stavroula and Lilienfeld, Scott O. and Lindsay, D. Stephen and Morey, Candice C. and Munafò, Marcus and Newell, Benjamin R. and Pashler, Harold and Shanks, David R. and Simons, Daniel J. and Wicherts, Jelte M. and Albarracin, Dolores and Anderson, Nicole D. and Antonakis, John and Arkes, Hal R. and Back, Mitja D. and Banks, George C. and Beevers, Christopher and Bennett, Andrew A. and Bleidorn, Wiebke and Boyer, Ty W. and Cacciari, Cristina and Carter, Alice S. and Cesario, Joseph and Clifton, Charles and Conroy, Ronán M. and Cortese, Mike and Cosci, Fiammetta and Cowan, Nelson and Crawford, Jarret and Crone, Eveline A. and Curtin, John and Engle, Randall and Farrell, Simon and Fearon, Pasco and Fichman, Mark and Frankenhuis, Willem and Freund, Alexandra M. and Gaskell, M. Gareth and Giner-Sorolla, Roger and Green, Don P. and Greene, Robert L. and Harlow, Lisa L. and de la Guardia, Fernando Hoces and Isaacowitz, Derek and Kolodner, Janet and Lieberman, Debra and Logan, Gordon D. and Mendes, Wendy B. and Moersdorf, Lea and Nyhan, Brendan and Pollack, Jeffrey and Sullivan, Christopher and Vazire, Simine and Wagenmakers, Eric-Jan},
  date = {2019-12-02},
  journaltitle = {Nature Human Behaviour},
  shortjournal = {Nat Hum Behav},
  pages = {1--3},
  issn = {2397-3374},
  doi = {10.1038/s41562-019-0772-6},
  url = {https://www.nature.com/articles/s41562-019-0772-6},
  urldate = {2020-01-08},
  abstract = {We present a consensus-based checklist to improve and document the transparency of research reports in social and behavioural research. An accompanying online application allows users to complete the form and generate a report that they can submit with their manuscript or post to a public repository.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\5T92PG4Y\\Aczel et al_2019_A consensus-based transparency checklist.pdf;C\:\\Users\\lissa102\\Zotero\\storage\\ZLCD57B9\\s41562-019-0772-6.html},
  langid = {english}
}

@article{adolphOpenBehavioralScience2012,
  title = {Toward {{Open Behavioral Science}}},
  author = {Adolph, Karen E. and Gilmore, Rick O. and Freeman, Clinton and Sanderson, Penelope and Millman, David},
  date = {2012-07},
  journaltitle = {Psychological Inquiry},
  shortjournal = {Psychological Inquiry},
  volume = {23},
  pages = {244--247},
  issn = {1047-840X, 1532-7965},
  doi = {10.1080/1047840X.2012.705133},
  url = {http://www.tandfonline.com/doi/abs/10.1080/1047840X.2012.705133},
  urldate = {2019-08-30},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\PYPE6J4K\\Adolph et al. - 2012 - Toward Open Behavioral Science.pdf},
  keywords = {open developmental science},
  langid = {english},
  number = {3}
}

@online{allaireMarkdownPythonEngine,
  title = {R {{Markdown Python Engine}}},
  author = {Allaire, J. J. and Ushey, Kevin and RStudio and Tang, Yuan},
  url = {https://rstudio.github.io/reticulate/articles/r_markdown.html},
  urldate = {2020-01-13},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\WSHC7J3P\\r_markdown.html}
}

@software{austPapajaPrepareReproducible2020,
  title = {Papaja: {{Prepare}} Reproducible {{APA}} Journal Articles with {{R Markdown}}.},
  author = {Aust, Frederik and Barth, Marius},
  date = {2020-01-28T22:33:21Z},
  origdate = {2014-07-20T11:43:41Z},
  url = {https://github.com/crsh/papaja},
  urldate = {2020-02-04},
  abstract = {papaja (Preparing APA Journal Articles) is an R package that provides document formats to produce complete APA manscripts from RMarkdown-files (PDF and Word documents) and helper functions that fac...},
  keywords = {apa,apa-guidelines,journal,manuscript,psychology,r,reproducible-paper,reproducible-research,rmarkdown},
  version = {0.1.0.9842}
}

@software{austPreregMarkdownTemplates2019,
  title = {Prereg: {{R Markdown Templates}} to {{Preregister Scientific Studies}}},
  shorttitle = {Prereg},
  author = {Aust, Frederik},
  date = {2019-01-09},
  url = {https://CRAN.R-project.org/package=prereg},
  urldate = {2020-01-30},
  abstract = {Provides a collection of templates to author preregistration documents for scientific studies in PDF format.},
  version = {0.4.0}
}

@article{bauerExpandingReachPsychological2019,
  title = {Expanding the {{Reach}} of {{Psychological Science}}},
  author = {Bauer, Patricia J.},
  date = {2019-12-18},
  journaltitle = {Psychological Science},
  shortjournal = {Psychol Sci},
  pages = {0956797619898664},
  issn = {0956-7976},
  doi = {10.1177/0956797619898664},
  url = {https://doi.org/10.1177/0956797619898664},
  urldate = {2020-01-08},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\RIZC4MV4\\2019_Expanding the Reach of Psychological Science.pdf},
  langid = {english}
}

@article{blischakQuickIntroductionVersion2016,
  title = {A {{Quick Introduction}} to {{Version Control}} with {{Git}} and {{GitHub}}},
  author = {Blischak, John D. and Davenport, Emily R. and Wilson, Greg},
  date = {2016-01-19},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  volume = {12},
  pages = {e1004668},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004668},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004668},
  urldate = {2019-10-26},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\NYSUERQ3\\Blischak et al_2016_A Quick Introduction to Version Control with Git and GitHub.pdf;C\:\\Users\\lissa102\\Zotero\\storage\\BCILHZ8S\\article.html},
  keywords = {Cloning,Control systems,Graphical user interface,Kidneys,Machine learning,Morphology (linguistics),Scientists,Software development},
  langid = {english},
  number = {1}
}

@online{brownHowLearnedStop2017,
  title = {How {{I}} Learned to Stop Worrying and Love the Coming Archivability Crisis in Scientific Software},
  author = {Brown, C. Titus},
  date = {2017},
  url = {http://ivory.idyll.org/blog/2017-pof-software-archivability.html},
  urldate = {2020-01-13},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\LRI2PYYL\\2017-pof-software-archivability.html}
}

@article{coyneReplicationInitiativesWill2016,
  title = {Replication Initiatives Will Not Salvage the Trustworthiness of Psychology},
  author = {Coyne, James C.},
  date = {2016-05-31},
  journaltitle = {BMC Psychology},
  shortjournal = {BMC Psychology},
  volume = {4},
  pages = {28},
  issn = {2050-7283},
  doi = {10.1186/s40359-016-0134-3},
  url = {https://doi.org/10.1186/s40359-016-0134-3},
  urldate = {2019-08-31},
  abstract = {Replication initiatives in psychology continue to gather considerable attention from far outside the field, as well as controversy from within. Some accomplishments of these initiatives are noted, but this article focuses on why they do not provide a general solution for what ails psychology. There are inherent limitations to mass replications ever being conducted in many areas of psychology, both in terms of their practicality and their prospects for improving the science. Unnecessary compromises were built into the ground rules for design and publication of the Open Science Collaboration: Psychology that undermine its effectiveness. Some ground rules could actually be flipped into guidance for how not to conduct replications. Greater adherence to best publication practices, transparency in the design and publishing of research, strengthening of independent post-publication peer review and firmer enforcement of rules about data sharing and declarations of conflict of interest would make many replications unnecessary. Yet, it has been difficult to move beyond simple endorsement of these measures to consistent implementation. Given the strong institutional support for questionable publication practices, progress will depend on effective individual and collective use of social media to expose lapses and demand reform. Some recent incidents highlight the necessity of this.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\FBLTNNET\\Coyne - 2016 - Replication initiatives will not salvage the trust.pdf},
  keywords = {open developmental science,replication},
  number = {1}
}

@online{EditorsPsychOpen,
  title = {For {{Editors}}: {{PsychOpen}}},
  url = {https://www.psychopen.eu/for-editors/},
  urldate = {2020-01-08}
}

@article{gauMoreBrainDroppings,
  title = {More Brain Droppings on the Replication Crisis},
  author = {Gau, Remi},
  pages = {52},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\IEXXQU7X\\Gau - More brain droppings on the replication crisis.pdf},
  langid = {english}
}

@article{gelmanStatisticalCrisisScience2014,
  title = {The Statistical Crisis in Science: Data-Dependent Analysis--a \&quot;Garden of Forking Paths\&quot;--Explains Why Many Statistically Significant Comparisons Don't Hold Up},
  shorttitle = {The Statistical Crisis in Science},
  author = {Gelman, Andrew and Loken, Eric},
  date = {2014-11-01},
  journaltitle = {American Scientist},
  volume = {102},
  pages = {460--466},
  issn = {00030996},
  url = {https://go.gale.com/ps/i.do?p=AONE&sw=w&issn=00030996&v=2.1&it=r&id=GALE%7CA389260653&sid=googleScholar&linkaccess=abs},
  urldate = {2020-02-08},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\M37N35RL\\anonymous.html},
  langid = {english},
  number = {6}
}

@software{hallquistMplusAutomationPackageFacilitating2018,
  title = {{{MplusAutomation}}: {{An R Package}} for {{Facilitating Large}}-{{Scale Latent Variable Analyses}} in {{Mplus}}},
  shorttitle = {{{MplusAutomation}}},
  author = {Hallquist, Michael and Wiley, Joshua and van Lissa, Caspar},
  date = {2018-11-25},
  url = {https://CRAN.R-project.org/package=MplusAutomation},
  urldate = {2020-02-05},
  abstract = {Leverages the R language to automate latent variable model estimation and interpretation using 'Mplus', a powerful latent variable modeling program developed by Muthen and Muthen ({$<$}http://www.statmodel.com{$>$}). Specifically, this package provides routines for creating related groups of models, running batches of models, and extracting and tabulating model parameters and fit statistics.},
  keywords = {Psychometrics},
  version = {0.7-3}
}

@article{helgessonResponsibilityScientificMisconduct2018,
  title = {Responsibility for Scientific Misconduct in Collaborative Papers},
  author = {Helgesson, Gert and Eriksson, Stefan},
  date = {2018-09-01},
  journaltitle = {Medicine, Health Care and Philosophy},
  shortjournal = {Med Health Care and Philos},
  volume = {21},
  pages = {423--430},
  issn = {1572-8633},
  doi = {10.1007/s11019-017-9817-7},
  url = {https://doi.org/10.1007/s11019-017-9817-7},
  urldate = {2019-10-25},
  abstract = {This paper concerns the responsibility of co-authors in cases of scientific misconduct. Arguments in research integrity guidelines and in the bioethics literature concerning authorship responsibilities are discussed. It is argued that it is unreasonable to claim that for every case where a research paper is found to be fraudulent, each author is morally responsible for all aspects of that paper, or that one particular author has such a responsibility. It is further argued that it is more constructive to specify what task responsibilities come with different roles in a project and describe what kinds of situations or events call for some kind of action, and what the appropriate actions might be.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\SK7V3ZIB\\Helgesson_Eriksson_2018_Responsibility for scientific misconduct in collaborative papers.pdf},
  keywords = {Accountability,Authorship,Research ethics,Research integrity,Responsibility,Scientific misconduct},
  langid = {english},
  number = {3}
}

@article{johnMeasuringPrevalenceQuestionable2012,
  title = {Measuring the {{Prevalence}} of {{Questionable Research Practices With Incentives}} for {{Truth Telling}}},
  author = {John, Leslie K. and Loewenstein, George and Prelec, Drazen},
  date = {2012-05-01},
  journaltitle = {Psychological Science},
  shortjournal = {Psychol Sci},
  volume = {23},
  pages = {524--532},
  issn = {0956-7976},
  doi = {10.1177/0956797611430953},
  url = {https://doi.org/10.1177/0956797611430953},
  urldate = {2020-02-08},
  abstract = {Cases of clear scientific misconduct have received significant media attention recently, but less flagrantly questionable research practices may be more prevalent and, ultimately, more damaging to the academic enterprise. Using an anonymous elicitation format supplemented by incentives for honest reporting, we surveyed over 2,000 psychologists about their involvement in questionable research practices. The impact of truth-telling incentives on self-admissions of questionable research practices was positive, and this impact was greater for practices that respondents judged to be less defensible. Combining three different estimation methods, we found that the percentage of respondents who have engaged in questionable practices was surprisingly high. This finding suggests that some questionable practices may constitute the prevailing research norm.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\TJKTAI5M\\John et al_2012_Measuring the Prevalence of Questionable Research Practices With Incentives for.pdf},
  keywords = {disclosure,judgment,methodology,professional standards},
  langid = {english},
  number = {5}
}

@article{kerrHARKingHypothesizingResults1998,
  title = {{{HARKing}}: Hypothesizing after the Results Are Known},
  shorttitle = {{{HARKing}}},
  author = {Kerr, N. L.},
  date = {1998},
  journaltitle = {Personality and Social Psychology Review: An Official Journal of the Society for Personality and Social Psychology, Inc},
  shortjournal = {Pers Soc Psychol Rev},
  volume = {2},
  pages = {196--217},
  issn = {1088-8683},
  doi = {10.1207/s15327957pspr0203_4},
  abstract = {This article considers a practice in scientific communication termed HARKing (Hypothesizing After the Results are Known). HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in one's research report as i f it were, in fact, an a priori hypotheses. Several forms of HARKing are identified and survey data are presented that suggests that at least some forms of HARKing are widely practiced and widely seen as inappropriate. I identify several reasons why scientists might HARK. Then I discuss several reasons why scientists ought not to HARK. It is conceded that the question of whether HARKing ' s costs exceed its benefits is a complex one that ought to be addressed through research, open discussion, and debate. To help stimulate such discussion (and for those such as myself who suspect that HARKing's costs do exceed its benefits), I conclude the article with some suggestions for deterring HARKing.},
  eprint = {15647155},
  eprinttype = {pmid},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\N57L34VT\\Kerr - 1998 - HARKing hypothesizing after the results are known.pdf},
  langid = {english},
  number = {3}
}

@article{kidwellBadgesAcknowledgeOpen2016,
  title = {Badges to {{Acknowledge Open Practices}}: {{A Simple}}, {{Low}}-{{Cost}}, {{Effective Method}} for {{Increasing Transparency}}},
  shorttitle = {Badges to {{Acknowledge Open Practices}}},
  author = {Kidwell, Mallory C. and Lazarević, Ljiljana B. and Baranski, Erica and Hardwicke, Tom E. and Piechowski, Sarah and Falkenberg, Lina-Sophia and Kennett, Curtis and Slowik, Agnieszka and Sonnleitner, Carina and Hess-Holden, Chelsey and Errington, Timothy M. and Fiedler, Susann and Nosek, Brian A.},
  date = {2016-05-12},
  journaltitle = {PLOS Biology},
  shortjournal = {PLOS Biology},
  volume = {14},
  pages = {e1002456},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.1002456},
  url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002456},
  urldate = {2019-08-31},
  abstract = {Beginning January 2014, Psychological Science gave authors the opportunity to signal open data and materials if they qualified for badges that accompanied published articles. Before badges, less than 3\% of Psychological Science articles reported open data. After badges, 23\% reported open data, with an accelerating trend; 39\% reported open data in the first half of 2015, an increase of more than an order of magnitude from baseline. There was no change over time in the low rates of data sharing among comparison journals. Moreover, reporting openness does not guarantee openness. When badges were earned, reportedly available data were more likely to be actually available, correct, usable, and complete than when badges were not earned. Open materials also increased to a weaker degree, and there was more variability among comparison journals. Badges are simple, effective signals to promote open practices and improve preservation of data and materials by using independent repositories.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\35AQ9U5U\\Kidwell et al. - 2016 - Badges to Acknowledge Open Practices A Simple, Lo.pdf;C\:\\Users\\lissa102\\Zotero\\storage\\NGFUB647\\article.html},
  keywords = {Behavior,Cognitive psychology,Experimental psychology,Open data,open developmental science,Open science,Psychology,Research assessment,Scientific publishing},
  langid = {english},
  number = {5}
}

@article{klebelPeerReviewPreprint2020,
  title = {Peer Review and Preprint Policies Are Unclear at Most Major Journals},
  author = {Klebel, Thomas and Reichmann, Stefan and Polka, Jessica and McDowell, Gary and Penfold, Naomi and Hindle, Samantha and Ross-Hellauer, Tony},
  date = {2020-01-30},
  journaltitle = {bioRxiv},
  pages = {2020.01.24.918995},
  doi = {10.1101/2020.01.24.918995},
  url = {https://www.biorxiv.org/content/10.1101/2020.01.24.918995v1},
  urldate = {2020-02-05},
  abstract = {{$<$}p{$>$}Clear and findable publishing policies are important for authors to choose appropriate journals for publication. We investigated the clarity of policies of 171 major academic journals across disciplines regarding peer review and preprinting. 31.6\% of journals surveyed do not provide information on the type of peer review they use. Information on whether preprints can be posted or not is unclear in 39.2\% of journals. 58.5\% of journals offer no clear information on whether reviewer identities are revealed to authors. Around 75\% of journals have no clear policy on coreviewing, citation of preprints, and publication of reviewer identities. Information regarding practices of Open Peer Review is even more scarce, with \&lt;20\% of journals providing clear information. Having found a lack of clear information, we conclude by examining the implications this has for researchers (especially early career) and the spread of open research practices.{$<$}/p{$>$}},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\CZ5JNZPP\\Klebel et al_2020_Peer review and preprint policies are unclear at most major journals.pdf;C\:\\Users\\lissa102\\Zotero\\storage\\LCSN629R\\2020.01.24.html},
  langid = {english}
}

@article{kulkeImplicitTheoryMind2018,
  title = {Is {{Implicit Theory}} of {{Mind}} a {{Real}} and {{Robust Phenomenon}}? {{Results From}} a {{Systematic Replication Study}}},
  shorttitle = {Is {{Implicit Theory}} of {{Mind}} a {{Real}} and {{Robust Phenomenon}}?},
  author = {Kulke, Louisa and von Duhn, Britta and Schneider, Dana and Rakoczy, Hannes},
  date = {2018-06},
  journaltitle = {Psychological Science},
  shortjournal = {Psychol Sci},
  volume = {29},
  pages = {888--900},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797617747090},
  url = {http://journals.sagepub.com/doi/10.1177/0956797617747090},
  urldate = {2019-08-31},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\HRUN9XAA\\Kulke et al. - 2018 - Is Implicit Theory of Mind a Real and Robust Pheno.pdf},
  keywords = {open developmental science,replication},
  langid = {english},
  number = {6},
  options = {useprefix=true}
}

@report{leveltFailingScienceFraudulent2012,
  title = {Failing Science: {{The}} Fraudulent Research Practices of Social Psychologist {{Diederik Stapel}} ({{Falende}} Wetenschap: {{De}} Frauduleuze Onderzoekspraktijken van Sociaal-Psycholoog {{Diederik Stapel}})},
  author = {Levelt, J. M., Willem and Noort,, E. and Drenth, P. J. D.},
  date = {2012},
  url = {https://www.onderwijsbrabant.nl/sites/default/files/eindrapport_stapel_nov_2012.pdf},
  urldate = {2020-02-08},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\9R88L72R\\eindrapport_stapel_nov_2012.pdf}
}

@article{lindsayResearchPreregistration1012016,
  title = {Research {{Preregistration}} 101},
  author = {Lindsay, D. Stephen and Simons, Daniel J. and Lilienfeld, Scott O.},
  date = {2016-11-30},
  journaltitle = {APS Observer},
  volume = {29},
  url = {https://www.psychologicalscience.org/observer/research-preregistration-101},
  urldate = {2020-01-30},
  abstract = {Psychological Science Editor in Chief D. Stephen Lindsay, Clinical Psychological Science Editor Scott O. Lilienfeld, and APS Fellow Daniel J. Simons explain the rationale for and benefits of preregistration, for researchers and for the field of psychological science at large.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\BVSBXU6L\\research-preregistration-101.html},
  langid = {american},
  number = {10}
}

@article{martinArePsychologyJournals2017,
  title = {Are {{Psychology Journals Anti}}-Replication? {{A Snapshot}} of {{Editorial Practices}}},
  shorttitle = {Are {{Psychology Journals Anti}}-Replication?},
  author = {Martin, G. N. and Clarke, Richard M.},
  date = {2017},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {8},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2017.00523},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2017.00523/full},
  urldate = {2019-08-31},
  abstract = {Recent research in psychology has highlighted a number of replication problems in the discipline, with publication bias – the preference for publishing original and positive results, and a resistance to publishing negative results and replications- identified as one reason for replication failure. However, little empirical research exists to demonstrate that journals explicitly refuse to publish replications. We reviewed the instructions to authors and the published aims of 1151 psychology journals and examined whether they indicated that replications were permitted and accepted. We also examined whether journal practices differed across branches of the discipline, and whether editorial practices differed between low and high impact journals. Thirty three journals (3\%) stated in their aims or instructions to authors that they accepted replications. There was no difference between high and low impact journals. The implications of these findings for psychology are discussed.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\IB4FH5YA\\Martin and Clarke - 2017 - Are Psychology Journals Anti-replication A Snapsh.pdf},
  keywords = {JOURNAL EDITORIAL PRACTICES,open developmental science,p-hacking,Psychology,Publication Bias,Replication},
  langid = {english}
}

@article{mclean2019empirical,
  title = {The Empirical Structure of Narrative Identity: {{The}} Initial {{Big Three}}.},
  author = {McLean, Kate C and Syed, Moin and Pasupathi, Monisha and Adler, Jonathan M and Dunlop, William L and Drustrup, David and Fivush, Robyn and Graci, Matthew E and Lilgendahl, Jennifer P and Lodi-Smith, Jennifer and others},
  date = {2019},
  journaltitle = {Journal of personality and social psychology},
  publisher = {{American Psychological Association}},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\XABPPFSJ\\McLean,etal,NarrativeStructure-inpressJPSP.pdf}
}

@article{moreauMetaanalysisTemplatesMaterials2019,
  title = {Meta-Analysis Templates and Materials},
  author = {Moreau, David and Gamble, Beau},
  date = {2019-11-28},
  doi = {None},
  url = {https://osf.io/q8stz/},
  urldate = {2020-01-08},
  abstract = {Hosted on the Open Science Framework},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\ZI3WLI64\\q8stz.html},
  langid = {english}
}

@online{muenchenPopularityDataScience2012,
  title = {The {{Popularity}} of {{Data Science Software}}},
  author = {Muenchen, Robert A.},
  date = {2012-04-25T23:22:32+00:00},
  journaltitle = {r4stats.com},
  url = {http://r4stats.com/articles/popularity/},
  urldate = {2020-01-08},
  abstract = {by Abstract This article, formerly known as The Popularity of Data Analysis Software, presents various ways of measuring the popularity or market share of software for advanced a…},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\WEGIDWCV\\popularity.html},
  langid = {american}
}

@book{national2009being,
  title = {On Being a Scientist: A Guide to Responsible Conduct in Research},
  author = {National Academy of Sciences},
  date = {2009},
  edition = {3},
  publisher = {{National Academies Press (US)}},
  location = {{Washington, DC, US}}
}

@article{nosekPromotingOpenResearch2015a,
  title = {Promoting an Open Research Culture},
  author = {Nosek, B. A. and Alter, G. and Banks, G. C. and Borsboom, D. and Bowman, S. D. and Breckler, S. J. and Buck, S. and Chambers, C. D. and Chin, G. and Christensen, G. and Contestabile, M. and Dafoe, A. and Eich, E. and Freese, J. and Glennerster, R. and Goroff, D. and Green, D. P. and Hesse, B. and Humphreys, M. and Ishiyama, J. and Karlan, D. and Kraut, A. and Lupia, A. and Mabry, P. and Madon, T. and Malhotra, N. and Mayo-Wilson, E. and McNutt, M. and Miguel, E. and Paluck, E. Levy and Simonsohn, U. and Soderberg, C. and Spellman, B. A. and Turitto, J. and VandenBos, G. and Vazire, S. and Wagenmakers, E. J. and Wilson, R. and Yarkoni, T.},
  date = {2015-06-26},
  journaltitle = {Science},
  volume = {348},
  pages = {1422--1425},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aab2374},
  url = {https://science.sciencemag.org/content/348/6242/1422},
  urldate = {2019-09-01},
  abstract = {Author guidelines for journals could help to promote transparency, openness, and reproducibility
Author guidelines for journals could help to promote transparency, openness, and reproducibility},
  eprint = {26113702},
  eprinttype = {pmid},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\7JEB688L\\Nosek et al_2015_Promoting an open research culture.pdf;C\:\\Users\\lissa102\\Zotero\\storage\\ZKLD8GV8\\1422.html},
  langid = {english},
  number = {6242}
}

@article{nosekScientificUtopiaOpening2012,
  title = {Scientific {{Utopia}}: {{I}}. {{Opening Scientific Communication}}},
  shorttitle = {Scientific {{Utopia}}},
  author = {Nosek, Brian A. and Bar-Anan, Yoav},
  date = {2012-07-01},
  journaltitle = {Psychological Inquiry},
  volume = {23},
  pages = {217--243},
  issn = {1047-840X},
  doi = {10.1080/1047840X.2012.692215},
  url = {https://doi.org/10.1080/1047840X.2012.692215},
  urldate = {2020-02-05},
  abstract = {Existing norms for scientific communication are rooted in anachronistic practices of bygone eras making them needlessly inefficient. We outline a path that moves away from the existing model of scientific communication to improve the efficiency in meeting the purpose of public science—knowledge accumulation. We call for six changes: (a) full embrace of digital communication; (b) open access to all published research; (c) disentangling publication from evaluation; (d) breaking the “one article, one journal” model with a grading system for evaluation and diversified dissemination outlets; (e) publishing peer review; and (f) allowing open, continuous peer review. We address conceptual and practical barriers to change and provide examples showing how the suggested practices are being used already. The critical barriers to change are not technical or financial; they are social. Although scientists guard the status quo, they also have the power to change it.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\PK3KU64G\\Nosek_Bar-Anan_2012_Scientific Utopia.pdf;C\:\\Users\\lissa102\\Zotero\\storage\\JTSJWCMS\\1047840X.2012.html},
  number = {3}
}

@article{peikertReproducibleDataAnalysis2019,
  title = {A {{Reproducible Data Analysis Workflow}} with {{R Markdown}}, {{Git}}, {{Make}}, and {{Docker}}},
  author = {Peikert, Aaron and Brandmaier, Andreas Markus},
  date = {2019-11-11T15:08:01.530Z},
  doi = {10.31234/osf.io/8xzqy},
  url = {https://psyarxiv.com/8xzqy/},
  urldate = {2020-01-09},
  abstract = {In this tutorial, we describe a workflow to ensure long-term reproducibility of R-based data analyses. The workflow leverages established tools and practices from software engineering. It combines the benefits of various open-source software tools including R Markdown, Git, Make, and Docker, whose interplay ensures seamless integration of version management, dynamic report generation conforming to various journal styles and full cross-platform and long-term computational reproducibility. The workflow ensures meeting the primary goals that 1) the reporting of statistical results is consistent with the actual statistical results (dynamic report generation), the analysis exactly reproduces at a later time even if the computing platform or software is changed (computational reproducibility), and 3) changes at any time (during development and post-publication) are tracked, tagged, and documented while earlier versions of both data and code remain accessible.  While the research community increasingly recognizes dynamic document generation and version management as tools to ensure reproducibility, we demonstrate with practical examples that these alone are not sufficient to ensure long-term computational reproducibility. Leveraging containerization, dependence management, version management, and literate programming, the workflow increases scientific productivity by facilitating later reproducibility and reuse of code and data.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\AS9JL74K\\Peikert_Brandmaier_2019_A Reproducible Data Analysis Workflow with R Markdown, Git, Make, and Docker.pdf;C\:\\Users\\lissa102\\Zotero\\storage\\2NT8MMMG\\8xzqy.html}
}

@Manual{repro,
  title = {repro: Automated Setup of Reproducible Workflows and their Dependencies},
  author = {Peikert, Aaron and Brandmaier, Andreas Markus and {van Lissa}, {Caper J.}},
  year = {2020},
  note = {R package version 0.1.0},
  url = {https://github.com/aaronpeikert/repro},
}

@online{pengSimpleExplanationReplication2016,
  title = {A {{Simple Explanation}} for the {{Replication Crisis}} in {{Science}} · {{Simply Statistics}}},
  author = {Peng, Roger},
  date = {2016-08-24},
  journaltitle = {Simplystats},
  url = {https://simplystatistics.org/2016/08/24/replication-crisis/},
  urldate = {2019-09-01},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\CMKAL9TN\\replication-crisis.html}
}

@article{plesserReproducibilityVsReplicability2018,
  title = {Reproducibility vs. {{Replicability}}: {{A Brief History}} of a {{Confused Terminology}}},
  shorttitle = {Reproducibility vs. {{Replicability}}},
  author = {Plesser, Hans E.},
  date = {2018},
  journaltitle = {Frontiers in Neuroinformatics},
  shortjournal = {Front. Neuroinform.},
  volume = {11},
  issn = {1662-5196},
  doi = {10.3389/fninf.2017.00076},
  url = {https://www.frontiersin.org/articles/10.3389/fninf.2017.00076/full},
  urldate = {2019-09-01},
  abstract = {Reproducibility vs. Replicability: A Brief History of a Confused Terminology},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\VSY3D595\\Plesser_2018_Reproducibility vs.pdf},
  keywords = {artifacts,computational science,repeatability,replicability,reproducibility},
  langid = {english}
}

@article{ramGitCanFacilitate2013,
  title = {Git Can Facilitate Greater Reproducibility and Increased Transparency in Science},
  author = {Ram, Karthik},
  date = {2013-02-28},
  journaltitle = {Source Code for Biology and Medicine},
  shortjournal = {Source Code for Biology and Medicine},
  volume = {8},
  pages = {7},
  issn = {1751-0473},
  doi = {10.1186/1751-0473-8-7},
  url = {https://doi.org/10.1186/1751-0473-8-7},
  urldate = {2020-01-08},
  abstract = {Reproducibility is the hallmark of good science. Maintaining a high degree of transparency in scientific reporting is essential not just for gaining trust and credibility within the scientific community but also for facilitating the development of new ideas. Sharing data and computer code associated with publications is becoming increasingly common, motivated partly in response to data deposition requirements from journals and mandates from funders. Despite this increase in transparency, it is still difficult to reproduce or build upon the findings of most scientific publications without access to a more complete workflow.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\RZB63RDE\\Ram_2013_Git can facilitate greater reproducibility and increased transparency in science.pdf;C\:\\Users\\lissa102\\Zotero\\storage\\H4UH8INM\\1751-0473-8-7.html},
  number = {1}
}

@online{ReproducibleWorkflowVersion,
  title = {Reproducible Workflow and Version Control with {{Git}} and {{Github}}},
  url = {https://jules32.github.io/2016-07-12-Oxford/git/},
  urldate = {2019-10-26},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\7J672FN6\\git.html}
}

@article{rooyenEffectPeerReview2010,
  title = {Effect on Peer Review of Telling Reviewers That Their Signed Reviews Might Be Posted on the Web: Randomised Controlled Trial},
  shorttitle = {Effect on Peer Review of Telling Reviewers That Their Signed Reviews Might Be Posted on the Web},
  author = {van Rooyen, Susan and Delamothe, Tony and Evans, Stephen J. W.},
  date = {2010-11-16},
  journaltitle = {BMJ},
  shortjournal = {BMJ},
  volume = {341},
  pages = {c5729},
  issn = {0959-8138, 1468-5833},
  doi = {10.1136/bmj.c5729},
  url = {https://www.bmj.com/content/341/bmj.c5729},
  urldate = {2019-10-25},
  abstract = {Objectives To see whether telling peer reviewers that their signed reviews of original research papers might be posted on the BMJ’s website would affect the quality of their reviews.
Design Randomised controlled trial.
Setting A large international general medical journal based in the United Kingdom.
Participants 541 authors, 471 peer reviewers, and 12 editors.
Intervention Consecutive eligible papers were randomised either to have the reviewer’s signed report made available on the BMJ’s website alongside the published paper (intervention group) or to have the report made available only to the author—the BMJ’s normal procedure (control group). The intervention was the act of revealing to reviewers—after they had agreed to review but before they undertook their review—that their signed report might appear on the website.
Main outcome measures The main outcome measure was the quality of the reviews, as independently rated on a scale of 1 to 5 using a validated instrument by two editors and the corresponding author. Authors and editors were blind to the intervention group. Authors rated review quality before the fate of their paper had been decided. Additional outcomes were the time taken to complete the review and the reviewer’s recommendation regarding publication.
Results 558 manuscripts were randomised, and 471 manuscripts remained after exclusions. Of the 1039 reviewers approached to take part in the study, 568 (55\%) declined. Two editors’ evaluations of the quality of the peer review were obtained for all 471 manuscripts, with the corresponding author’s evaluation obtained for 453. There was no significant difference in review quality between the intervention and control groups (mean difference for editors 0.04, 95\% CI −0.09 to 0.17; for authors 0.06, 95\% CI −0.09 to 0.20). Any possible difference in favour of the control group was well below the level regarded as editorially significant. Reviewers in the intervention group took significantly longer to review (mean difference 25 minutes, 95\% CI 3.0 to 47.0 minutes).
Conclusion Telling peer reviewers that their signed reviews might be available in the public domain on the BMJ’s website had no important effect on review quality. Although the possibility of posting reviews online was associated with a high refusal rate among potential peer reviewers and an increase in the amount of time taken to write a review, we believe that the ethical arguments in favour of open peer review more than outweigh these disadvantages.},
  eprint = {21081600},
  eprinttype = {pmid},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\4LP5VZPE\\Rooyen et al_2010_Effect on peer review of telling reviewers that their signed reviews might be.pdf},
  langid = {english}
}

@article{rosenbergTidyLPAPackageEasily2018,
  title = {{{tidyLPA}}: {{An R Package}} to {{Easily Carry Out Latent Profile Analysis}} ({{LPA}}) {{Using Open}}-{{Source}} or {{Commercial Software}}},
  shorttitle = {{{tidyLPA}}},
  author = {Rosenberg, Joshua and Beymer, Patrick and Anderson, Daniel and Van Lissa, Caspar J. and Schmidt, Jennifer},
  date = {2018-10-10},
  journaltitle = {Journal of Open Source Software},
  volume = {3},
  pages = {978},
  issn = {2475-9066},
  doi = {10.21105/joss.00978},
  url = {https://joss.theoj.org/papers/10.21105/joss.00978},
  urldate = {2020-02-05},
  abstract = {Researchers are often interested in identifying homogeneous subgroups within heterogeneous samples on the basis of a set of measures, such as profiles of individuals’ motivation (i.e., their values, competence beliefs, and achievement goals). Latent Profile Analysis (LPA) is a statistical method for identifying such groups, or latent profiles, and is a special case of the general mixture model where all measured variables are continuous (Harring \& Hodis, 2016; Pastor, Barron, Miller, \& Davis, 2007). The tidyLPA package allows users to specify different models that determine whether and how different parameters (i.e., means, variances, and covariances) are estimated, and to specify and compare different solutions based on the number of profiles extracted.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\JVGDVPE9\\Rosenberg et al. - 2018 - tidyLPA An R Package to Easily Carry Out Latent P.pdf},
  langid = {english},
  number = {30}
}

@article{ross-hellauerWhatOpenPeer2017,
  title = {What Is Open Peer Review? {{A}} Systematic Review},
  shorttitle = {What Is Open Peer Review?},
  author = {Ross-Hellauer, Tony},
  date = {2017-08-31},
  journaltitle = {F1000Research},
  shortjournal = {F1000Res},
  volume = {6},
  issn = {2046-1402},
  doi = {10.12688/f1000research.11369.2},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5437951/},
  urldate = {2019-10-25},
  abstract = {Background: “Open peer review” (OPR), despite being a major pillar of Open Science, has neither a standardized definition nor an agreed schema of its features and implementations. The literature reflects this, with numerous overlapping and contradictory definitions. While for some the term refers to peer review where the identities of both author and reviewer are disclosed to each other, for others it signifies systems where reviewer reports are published alongside articles. For others it signifies both of these conditions, and for yet others it describes systems where not only “invited experts” are able to comment. For still others, it includes a variety of combinations of these and other novel methods.,
Methods: Recognising the absence of a consensus view on what open peer review is, this article undertakes a systematic review of definitions of “open peer review” or “open review”, to create a corpus of 122 definitions. These definitions are systematically analysed to build a coherent typology of the various innovations in peer review signified by the term, and hence provide the precise technical definition currently lacking.,
Results: This quantifiable data yields rich information on the range and extent of differing definitions over time and by broad subject area. Quantifying definitions in this way allows us to accurately portray exactly how ambiguously the phrase “open peer review” has been used thus far, for the literature offers 22 distinct configurations of seven traits, effectively meaning that there are 22 different definitions of OPR in the literature reviewed.,
Conclusions: I propose a pragmatic definition of open peer review as an umbrella term for a number of overlapping ways that peer review models can be adapted in line with the aims of Open Science, including making reviewer and author identities open, publishing review reports and enabling greater participation in the peer review process.},
  eprint = {28580134},
  eprinttype = {pmid},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\IRD7VFHQ\\Ross-Hellauer_2017_What is open peer review.pdf},
  pmcid = {PMC5437951}
}

@software{rossRedocReversibleReproducible2020,
  title = {Redoc - {{Reversible Reproducible Documents}}},
  author = {Ross, Noam},
  date = {2020-01-30T09:19:37Z},
  origdate = {2018-12-01T21:10:44Z},
  url = {https://github.com/noamross/redoc},
  urldate = {2020-02-05},
  abstract = {Reversible Reproducible Documents. Contribute to noamross/redoc development by creating an account on GitHub.},
  version = {0.1.0.9000}
}

@article{shroutPsychologyScienceKnowledge2018,
  title = {Psychology, {{Science}}, and {{Knowledge Construction}}: {{Broadening Perspectives}} from the {{Replication Crisis}}},
  shorttitle = {Psychology, {{Science}}, and {{Knowledge Construction}}},
  author = {Shrout, Patrick E. and Rodgers, Joseph L.},
  date = {2018},
  journaltitle = {Annual Review of Psychology},
  volume = {69},
  pages = {487--510},
  doi = {10.1146/annurev-psych-122216-011845},
  url = {https://doi.org/10.1146/annurev-psych-122216-011845},
  urldate = {2019-08-31},
  abstract = {Psychology advances knowledge by testing statistical hypotheses using empirical observations and data. The expectation is that most statistically significant findings can be replicated in new data and in new laboratories, but in practice many findings have replicated less often than expected, leading to claims of a replication crisis. We review recent methodological literature on questionable research practices, meta-analysis, and power analysis to explain the apparently high rates of failure to replicate. Psychologists can improve research practices to advance knowledge in ways that improve replicability. We recommend that researchers adopt open science conventions of preregi-stration and full disclosure and that replication efforts be based on multiple studies rather than on a single replication attempt. We call for more sophisticated power analyses, careful consideration of the various influences on effect sizes, and more complete disclosure of nonsignificant as well as statistically significant findings.},
  eprint = {29300688},
  eprinttype = {pmid},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\VSW7FA4W\\Shrout and Rodgers - 2018 - Psychology, Science, and Knowledge Construction B.pdf},
  number = {1}
}

@article{stanleyReproducibleTablesPsychology2018,
  title = {Reproducible {{Tables}} in {{Psychology Using}} the {{apaTables Package}}},
  author = {Stanley, David J. and Spence, Jeffrey R.},
  date = {2018-09-01},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  shortjournal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  pages = {415--431},
  issn = {2515-2459},
  doi = {10.1177/2515245918773743},
  url = {https://doi.org/10.1177/2515245918773743},
  urldate = {2020-01-03},
  abstract = {Growing awareness of how susceptible research is to errors, coupled with well-documented replication failures, has caused psychological researchers to move toward open science and reproducible research. In this Tutorial, to facilitate reproducible psychological research, we present a tool that creates reproducible tables that follow the American Psychological Association’s (APA’s) style. Our tool, apaTables, automates the creation of APA-style tables for commonly used statistics and analyses in psychological research: correlations, multiple regressions (with and without blocks), standardized mean differences, N-way independent-groups analyses of variance (ANOVAs), within-subjects ANOVAs, and mixed-design ANOVAs. All tables are saved as Microsoft Word documents, so they can be readily incorporated into manuscripts without manual formatting or transcription of values.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\SYWRI2PD\\Stanley_Spence_2018_Reproducible Tables in Psychology Using the apaTables Package.pdf},
  keywords = {data sharing,open data,open materials,open science,R,replication,reproducibility,reproducible analyses,reproducible research,reproducible tables,statistical tools,transparency in research},
  langid = {english},
  number = {3}
}

@online{SupplementalUsingGit,
  title = {Supplemental: {{Using Git}} from {{RStudio}} – {{Version Control}} with {{Git}}},
  url = {https://swcarpentry.github.io/git-novice/14-supplemental-rstudio/index.html},
  urldate = {2019-10-26},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\7XNF7PKP\\index.html}
}

@unpublished{syedPromiseOpenScience2019,
  title = {The {{Promise}} of the {{Open Science Movement}} for {{Research}} on {{Identity}}},
  author = {Syed, Moin},
  date = {2019-05-14},
  url = {https://osf.io/7yb3s},
  urldate = {2019-08-31},
  abstract = {The open science movement has been gaining steam in numerous scientific disciplines (e.g., ecology, cancer biology, economics) as well as sub-disciplines of psychology (e.g., social, personality). These issues, however, have been scantly discussed in the context of identity research. This presentation will include an overview of core issues in the open science movement and how they apply to research on identity. Emphasis will be placed on how incorporating open science principles can improve both theoretical and empirical work on identity.},
  eventtitle = {International {{Society}} for {{Research}} on {{Identity}}},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\GTZZZWLT\\Syed - The Promise of the Open Science Movement for Resea.pdf},
  langid = {english},
  type = {Presidential address},
  venue = {{Naples, Italy}}
}

@online{TechBlogGitReproducibility,
  title = {{{TechBlog}}: {{Git}}: {{The}} Reproducibility Tool Scientists Love to Hate : {{Naturejobs Blog}}},
  url = {http://blogs.nature.com/naturejobs/2018/06/11/git-the-reproducibility-tool-scientists-love-to-hate/},
  urldate = {2019-10-26},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\UA5Y7BXZ\\git-the-reproducibility-tool-scientists-love-to-hate.html}
}

@article{vantveerPreregistrationSocialPsychology2016,
  title = {Pre-Registration in Social Psychology—{{A}} Discussion and Suggested Template},
  author = {van 't Veer, Anna Elisabeth and Giner-Sorolla, Roger},
  date = {2016-11},
  journaltitle = {Journal of Experimental Social Psychology},
  shortjournal = {Journal of Experimental Social Psychology},
  volume = {67},
  pages = {2--12},
  issn = {00221031},
  doi = {10.1016/j.jesp.2016.03.004},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0022103116301925},
  urldate = {2020-01-30},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\4CHWSIJB\\van 't Veer_Giner-Sorolla_2016_Pre-registration in social psychology—A discussion and suggested template.pdf},
  langid = {english},
  options = {useprefix=true}
}

@article{walshOpenPeerReview2000,
  title = {Open Peer Review: {{A}} Randomised Controlled Trial},
  shorttitle = {Open Peer Review},
  author = {Walsh, Elizabeth and Rooney, Maeve and Appleby, Louis and Wilkinson, Greg},
  date = {2000-01},
  journaltitle = {The British Journal of Psychiatry},
  volume = {176},
  pages = {47--51},
  issn = {0007-1250, 1472-1465},
  doi = {10.1192/bjp.176.1.47},
  url = {https://www.cambridge.org/core/journals/the-british-journal-of-psychiatry/article/open-peer-review-a-randomised-controlled-trial/1F81447FC67B3BAFDCCCCE82B6C7A187},
  urldate = {2019-10-25},
  abstract = {Background
Most scientific journals practise anonymous peer review. There is no evidence, however, that this is any better than an open system.

Aims
To evaluate the feasibility of an open peer review system.

Method
Reviewers for the British Journal of Psychiatry were asked whether they would agree to have their name revealed to the authors whose papers they review; 408 manuscripts assigned to reviewers who agreed were randomised to signed or unsigned groups. We measured review quality, tone, recommendation for publication and time taken to complete each review.

Results
A total of 245 reviewers (76\%) agreed to sign. Signed reviews were of higher quality, were more courteous and took longer to complete than unsigned reviews. Reviewers who signed were more likely to recommend publication.

Conclusions
This study supports the feasibility of an open peer review system and identifies such a system's potential drawbacks.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\LT9ETGKG\\Walsh et al_2000_Open peer review.pdf;C\:\\Users\\lissa102\\Zotero\\storage\\YZMR76SL\\1F81447FC67B3BAFDCCCCE82B6C7A187.html},
  langid = {english},
  number = {1}
}

@article{westonRecommendationsIncreasingTransparency2019,
  title = {Recommendations for {{Increasing}} the {{Transparency}} of {{Analysis}} of {{Preexisting Data Sets}}},
  shorttitle = {Recommendations for {{Increasing}} the {{Transparency}} of {{Analysis}} of {{Preexisting Data Sets}}},
  author = {Weston, Sara J. and Ritchie, Stuart J. and Rohrer, Julia M. and Przybylski, Andrew K.},
  date = {2019-06-11},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  doi = {10.1177/2515245919848684},
  url = {https://journals.sagepub.com/doi/10.1177/2515245919848684},
  urldate = {2020-01-23},
  abstract = {Secondary data analysis, or the analysis of preexisting data, provides a powerful tool for the resourceful psychological scientist. Never has this been more tru...},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\DE7WSWWM\\Weston et al_2019_Recommendations for Increasing the Transparency of Analysis of Preexisting Data.pdf;C\:\\Users\\lissa102\\Zotero\\storage\\IX2RRN5Z\\2515245919848684.html},
  langid = {english}
}

@online{ZombieLiterature,
  title = {The {{Zombie Literature}}},
  journaltitle = {The Scientist Magazine®},
  url = {https://www.the-scientist.com/features/the-zombie-literature-33627},
  urldate = {2019-10-25},
  abstract = {Retractions are on the rise. But reams of flawed research papers persist in the scientific literature. Is it time to change the way papers are published?},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\HQTCXNXY\\the-zombie-literature-33627.html},
  langid = {english}
}


