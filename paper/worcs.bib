
@article{adolphOpenBehavioralScience2012,
  langid = {english},
  title = {Toward {{Open Behavioral Science}}},
  volume = {23},
  issn = {1047-840X, 1532-7965},
  url = {http://www.tandfonline.com/doi/abs/10.1080/1047840X.2012.705133},
  doi = {10.1080/1047840X.2012.705133},
  number = {3},
  journaltitle = {Psychological Inquiry},
  shortjournal = {Psychological Inquiry},
  urldate = {2019-08-30},
  date = {2012-07},
  pages = {244-247},
  keywords = {open developmental science},
  author = {Adolph, Karen E. and Gilmore, Rick O. and Freeman, Clinton and Sanderson, Penelope and Millman, David},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\PYPE6J4K\\Adolph et al. - 2012 - Toward Open Behavioral Science.pdf}
}

@article{kulkeImplicitTheoryMind2018,
  langid = {english},
  title = {Is {{Implicit Theory}} of {{Mind}} a {{Real}} and {{Robust Phenomenon}}? {{Results From}} a {{Systematic Replication Study}}},
  volume = {29},
  issn = {0956-7976, 1467-9280},
  url = {http://journals.sagepub.com/doi/10.1177/0956797617747090},
  doi = {10.1177/0956797617747090},
  shorttitle = {Is {{Implicit Theory}} of {{Mind}} a {{Real}} and {{Robust Phenomenon}}?},
  number = {6},
  journaltitle = {Psychological Science},
  shortjournal = {Psychol Sci},
  urldate = {2019-08-31},
  date = {2018-06},
  pages = {888-900},
  keywords = {replication,open developmental science},
  author = {Kulke, Louisa and von Duhn, Britta and Schneider, Dana and Rakoczy, Hannes},
  options = {useprefix=true},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\HRUN9XAA\\Kulke et al. - 2018 - Is Implicit Theory of Mind a Real and Robust Pheno.pdf}
}

@article{coyneReplicationInitiativesWill2016,
  title = {Replication Initiatives Will Not Salvage the Trustworthiness of Psychology},
  volume = {4},
  issn = {2050-7283},
  url = {https://doi.org/10.1186/s40359-016-0134-3},
  doi = {10.1186/s40359-016-0134-3},
  abstract = {Replication initiatives in psychology continue to gather considerable attention from far outside the field, as well as controversy from within. Some accomplishments of these initiatives are noted, but this article focuses on why they do not provide a general solution for what ails psychology. There are inherent limitations to mass replications ever being conducted in many areas of psychology, both in terms of their practicality and their prospects for improving the science. Unnecessary compromises were built into the ground rules for design and publication of the Open Science Collaboration: Psychology that undermine its effectiveness. Some ground rules could actually be flipped into guidance for how not to conduct replications. Greater adherence to best publication practices, transparency in the design and publishing of research, strengthening of independent post-publication peer review and firmer enforcement of rules about data sharing and declarations of conflict of interest would make many replications unnecessary. Yet, it has been difficult to move beyond simple endorsement of these measures to consistent implementation. Given the strong institutional support for questionable publication practices, progress will depend on effective individual and collective use of social media to expose lapses and demand reform. Some recent incidents highlight the necessity of this.},
  number = {1},
  journaltitle = {BMC Psychology},
  shortjournal = {BMC Psychology},
  urldate = {2019-08-31},
  date = {2016-05-31},
  pages = {28},
  keywords = {replication,open developmental science},
  author = {Coyne, James C.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\FBLTNNET\\Coyne - 2016 - Replication initiatives will not salvage the trust.pdf}
}

@article{martinArePsychologyJournals2017,
  langid = {english},
  title = {Are {{Psychology Journals Anti}}-Replication? {{A Snapshot}} of {{Editorial Practices}}},
  volume = {8},
  issn = {1664-1078},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2017.00523/full},
  doi = {10.3389/fpsyg.2017.00523},
  shorttitle = {Are {{Psychology Journals Anti}}-Replication?},
  abstract = {Recent research in psychology has highlighted a number of replication problems in the discipline, with publication bias – the preference for publishing original and positive results, and a resistance to publishing negative results and replications- identified as one reason for replication failure. However, little empirical research exists to demonstrate that journals explicitly refuse to publish replications. We reviewed the instructions to authors and the published aims of 1151 psychology journals and examined whether they indicated that replications were permitted and accepted. We also examined whether journal practices differed across branches of the discipline, and whether editorial practices differed between low and high impact journals. Thirty three journals (3\%) stated in their aims or instructions to authors that they accepted replications. There was no difference between high and low impact journals. The implications of these findings for psychology are discussed.},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  urldate = {2019-08-31},
  date = {2017},
  keywords = {Psychology,Replication,JOURNAL EDITORIAL PRACTICES,p-hacking,Publication Bias,open developmental science},
  author = {Martin, G. N. and Clarke, Richard M.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\IB4FH5YA\\Martin and Clarke - 2017 - Are Psychology Journals Anti-replication A Snapsh.pdf}
}

@article{kidwellBadgesAcknowledgeOpen2016,
  langid = {english},
  title = {Badges to {{Acknowledge Open Practices}}: {{A Simple}}, {{Low}}-{{Cost}}, {{Effective Method}} for {{Increasing Transparency}}},
  volume = {14},
  issn = {1545-7885},
  url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002456},
  doi = {10.1371/journal.pbio.1002456},
  shorttitle = {Badges to {{Acknowledge Open Practices}}},
  abstract = {Beginning January 2014, Psychological Science gave authors the opportunity to signal open data and materials if they qualified for badges that accompanied published articles. Before badges, less than 3\% of Psychological Science articles reported open data. After badges, 23\% reported open data, with an accelerating trend; 39\% reported open data in the first half of 2015, an increase of more than an order of magnitude from baseline. There was no change over time in the low rates of data sharing among comparison journals. Moreover, reporting openness does not guarantee openness. When badges were earned, reportedly available data were more likely to be actually available, correct, usable, and complete than when badges were not earned. Open materials also increased to a weaker degree, and there was more variability among comparison journals. Badges are simple, effective signals to promote open practices and improve preservation of data and materials by using independent repositories.},
  number = {5},
  journaltitle = {PLOS Biology},
  shortjournal = {PLOS Biology},
  urldate = {2019-08-31},
  date = {2016-05-12},
  pages = {e1002456},
  keywords = {Behavior,Psychology,open developmental science,Cognitive psychology,Experimental psychology,Open data,Open science,Research assessment,Scientific publishing},
  author = {Kidwell, Mallory C. and Lazarević, Ljiljana B. and Baranski, Erica and Hardwicke, Tom E. and Piechowski, Sarah and Falkenberg, Lina-Sophia and Kennett, Curtis and Slowik, Agnieszka and Sonnleitner, Carina and Hess-Holden, Chelsey and Errington, Timothy M. and Fiedler, Susann and Nosek, Brian A.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\35AQ9U5U\\Kidwell et al. - 2016 - Badges to Acknowledge Open Practices A Simple, Lo.pdf;C\:\\Users\\lissa102\\Zotero\\storage\\NGFUB647\\article.html}
}

@article{shroutPsychologyScienceKnowledge2018,
  title = {Psychology, {{Science}}, and {{Knowledge Construction}}: {{Broadening Perspectives}} from the {{Replication Crisis}}},
  volume = {69},
  url = {https://doi.org/10.1146/annurev-psych-122216-011845},
  doi = {10.1146/annurev-psych-122216-011845},
  shorttitle = {Psychology, {{Science}}, and {{Knowledge Construction}}},
  abstract = {Psychology advances knowledge by testing statistical hypotheses using empirical observations and data. The expectation is that most statistically significant findings can be replicated in new data and in new laboratories, but in practice many findings have replicated less often than expected, leading to claims of a replication crisis. We review recent methodological literature on questionable research practices, meta-analysis, and power analysis to explain the apparently high rates of failure to replicate. Psychologists can improve research practices to advance knowledge in ways that improve replicability. We recommend that researchers adopt open science conventions of preregi-stration and full disclosure and that replication efforts be based on multiple studies rather than on a single replication attempt. We call for more sophisticated power analyses, careful consideration of the various influences on effect sizes, and more complete disclosure of nonsignificant as well as statistically significant findings.},
  number = {1},
  journaltitle = {Annual Review of Psychology},
  urldate = {2019-08-31},
  date = {2018},
  pages = {487-510},
  author = {Shrout, Patrick E. and Rodgers, Joseph L.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\VSW7FA4W\\Shrout and Rodgers - 2018 - Psychology, Science, and Knowledge Construction B.pdf},
  eprinttype = {pmid},
  eprint = {29300688}
}

@article{gauMoreBrainDroppings,
  langid = {english},
  title = {More Brain Droppings on the Replication Crisis},
  pages = {52},
  author = {Gau, Remi},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\IEXXQU7X\\Gau - More brain droppings on the replication crisis.pdf}
}

@unpublished{syedPromiseOpenScience2019,
  langid = {english},
  venue = {{Naples, Italy}},
  title = {The {{Promise}} of the {{Open Science Movement}} for {{Research}} on {{Identity}}},
  url = {https://osf.io/7yb3s},
  abstract = {The open science movement has been gaining steam in numerous scientific disciplines (e.g., ecology, cancer biology, economics) as well as sub-disciplines of psychology (e.g., social, personality). These issues, however, have been scantly discussed in the context of identity research. This presentation will include an overview of core issues in the open science movement and how they apply to research on identity. Emphasis will be placed on how incorporating open science principles can improve both theoretical and empirical work on identity.},
  type = {Presidential address},
  eventtitle = {International {{Society}} for {{Research}} on {{Identity}}},
  urldate = {2019-08-31},
  date = {2019-05-14},
  author = {Syed, Moin},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\GTZZZWLT\\Syed - The Promise of the Open Science Movement for Resea.pdf}
}

@article{kerrHARKingHypothesizingResults1998,
  langid = {english},
  title = {{{HARKing}}: Hypothesizing after the Results Are Known},
  volume = {2},
  issn = {1088-8683},
  doi = {10.1207/s15327957pspr0203_4},
  shorttitle = {{{HARKing}}},
  abstract = {This article considers a practice in scientific communication termed HARKing (Hypothesizing After the Results are Known). HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in one's research report as i f it were, in fact, an a priori hypotheses. Several forms of HARKing are identified and survey data are presented that suggests that at least some forms of HARKing are widely practiced and widely seen as inappropriate. I identify several reasons why scientists might HARK. Then I discuss several reasons why scientists ought not to HARK. It is conceded that the question of whether HARKing ' s costs exceed its benefits is a complex one that ought to be addressed through research, open discussion, and debate. To help stimulate such discussion (and for those such as myself who suspect that HARKing's costs do exceed its benefits), I conclude the article with some suggestions for deterring HARKing.},
  number = {3},
  journaltitle = {Personality and Social Psychology Review: An Official Journal of the Society for Personality and Social Psychology, Inc},
  shortjournal = {Pers Soc Psychol Rev},
  date = {1998},
  pages = {196-217},
  author = {Kerr, N. L.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\N57L34VT\\Kerr - 1998 - HARKing hypothesizing after the results are known.pdf},
  eprinttype = {pmid},
  eprint = {15647155}
}

@article{mclean2019empirical,
  title = {The Empirical Structure of Narrative Identity: {{The}} Initial {{Big Three}}.},
  journaltitle = {Journal of personality and social psychology},
  date = {2019},
  author = {McLean, Kate C and Syed, Moin and Pasupathi, Monisha and Adler, Jonathan M and Dunlop, William L and Drustrup, David and Fivush, Robyn and Graci, Matthew E and Lilgendahl, Jennifer P and Lodi-Smith, Jennifer and others},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\XABPPFSJ\\McLean,etal,NarrativeStructure-inpressJPSP.pdf},
  publisher = {{American Psychological Association}}
}

@article{nosekPromotingOpenResearch2015a,
  langid = {english},
  title = {Promoting an Open Research Culture},
  volume = {348},
  issn = {0036-8075, 1095-9203},
  url = {https://science.sciencemag.org/content/348/6242/1422},
  doi = {10.1126/science.aab2374},
  abstract = {Author guidelines for journals could help to promote transparency, openness, and reproducibility
Author guidelines for journals could help to promote transparency, openness, and reproducibility},
  number = {6242},
  journaltitle = {Science},
  urldate = {2019-09-01},
  date = {2015-06-26},
  pages = {1422-1425},
  author = {Nosek, B. A. and Alter, G. and Banks, G. C. and Borsboom, D. and Bowman, S. D. and Breckler, S. J. and Buck, S. and Chambers, C. D. and Chin, G. and Christensen, G. and Contestabile, M. and Dafoe, A. and Eich, E. and Freese, J. and Glennerster, R. and Goroff, D. and Green, D. P. and Hesse, B. and Humphreys, M. and Ishiyama, J. and Karlan, D. and Kraut, A. and Lupia, A. and Mabry, P. and Madon, T. and Malhotra, N. and Mayo-Wilson, E. and McNutt, M. and Miguel, E. and Paluck, E. Levy and Simonsohn, U. and Soderberg, C. and Spellman, B. A. and Turitto, J. and VandenBos, G. and Vazire, S. and Wagenmakers, E. J. and Wilson, R. and Yarkoni, T.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\7JEB688L\\Nosek et al_2015_Promoting an open research culture.pdf;C\:\\Users\\lissa102\\Zotero\\storage\\ZKLD8GV8\\1422.html},
  eprinttype = {pmid},
  eprint = {26113702}
}

@article{plesserReproducibilityVsReplicability2018,
  langid = {english},
  title = {Reproducibility vs. {{Replicability}}: {{A Brief History}} of a {{Confused Terminology}}},
  volume = {11},
  issn = {1662-5196},
  url = {https://www.frontiersin.org/articles/10.3389/fninf.2017.00076/full},
  doi = {10.3389/fninf.2017.00076},
  shorttitle = {Reproducibility vs. {{Replicability}}},
  abstract = {Reproducibility vs. Replicability: A Brief History of a Confused Terminology},
  journaltitle = {Frontiers in Neuroinformatics},
  shortjournal = {Front. Neuroinform.},
  urldate = {2019-09-01},
  date = {2018},
  keywords = {artifacts,computational science,repeatability,replicability,reproducibility},
  author = {Plesser, Hans E.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\VSY3D595\\Plesser_2018_Reproducibility vs.pdf}
}

@online{pengSimpleExplanationReplication2016,
  title = {A {{Simple Explanation}} for the {{Replication Crisis}} in {{Science}} · {{Simply Statistics}}},
  url = {https://simplystatistics.org/2016/08/24/replication-crisis/},
  journaltitle = {Simplystats},
  urldate = {2019-09-01},
  date = {2016-08-24},
  author = {Peng, Roger},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\CMKAL9TN\\replication-crisis.html}
}

@book{national2009being,
  location = {{Washington, DC, US}},
  title = {On Being a Scientist: A Guide to Responsible Conduct in Research},
  edition = {3},
  publisher = {{National Academies Press (US)}},
  date = {2009},
  author = {National Academy of Sciences}
}

@article{helgessonResponsibilityScientificMisconduct2018,
  langid = {english},
  title = {Responsibility for Scientific Misconduct in Collaborative Papers},
  volume = {21},
  issn = {1572-8633},
  url = {https://doi.org/10.1007/s11019-017-9817-7},
  doi = {10.1007/s11019-017-9817-7},
  abstract = {This paper concerns the responsibility of co-authors in cases of scientific misconduct. Arguments in research integrity guidelines and in the bioethics literature concerning authorship responsibilities are discussed. It is argued that it is unreasonable to claim that for every case where a research paper is found to be fraudulent, each author is morally responsible for all aspects of that paper, or that one particular author has such a responsibility. It is further argued that it is more constructive to specify what task responsibilities come with different roles in a project and describe what kinds of situations or events call for some kind of action, and what the appropriate actions might be.},
  number = {3},
  journaltitle = {Medicine, Health Care and Philosophy},
  shortjournal = {Med Health Care and Philos},
  urldate = {2019-10-25},
  date = {2018-09-01},
  pages = {423-430},
  keywords = {Accountability,Authorship,Research ethics,Research integrity,Responsibility,Scientific misconduct},
  author = {Helgesson, Gert and Eriksson, Stefan},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\SK7V3ZIB\\Helgesson_Eriksson_2018_Responsibility for scientific misconduct in collaborative papers.pdf}
}

@online{ZombieLiterature,
  langid = {english},
  title = {The {{Zombie Literature}}},
  url = {https://www.the-scientist.com/features/the-zombie-literature-33627},
  abstract = {Retractions are on the rise. But reams of flawed research papers persist in the scientific literature. Is it time to change the way papers are published?},
  journaltitle = {The Scientist Magazine®},
  urldate = {2019-10-25},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\HQTCXNXY\\the-zombie-literature-33627.html}
}

@article{rooyenEffectPeerReview2010,
  langid = {english},
  title = {Effect on Peer Review of Telling Reviewers That Their Signed Reviews Might Be Posted on the Web: Randomised Controlled Trial},
  volume = {341},
  issn = {0959-8138, 1468-5833},
  url = {https://www.bmj.com/content/341/bmj.c5729},
  doi = {10.1136/bmj.c5729},
  shorttitle = {Effect on Peer Review of Telling Reviewers That Their Signed Reviews Might Be Posted on the Web},
  abstract = {Objectives To see whether telling peer reviewers that their signed reviews of original research papers might be posted on the BMJ’s website would affect the quality of their reviews.
Design Randomised controlled trial.
Setting A large international general medical journal based in the United Kingdom.
Participants 541 authors, 471 peer reviewers, and 12 editors.
Intervention Consecutive eligible papers were randomised either to have the reviewer’s signed report made available on the BMJ’s website alongside the published paper (intervention group) or to have the report made available only to the author—the BMJ’s normal procedure (control group). The intervention was the act of revealing to reviewers—after they had agreed to review but before they undertook their review—that their signed report might appear on the website.
Main outcome measures The main outcome measure was the quality of the reviews, as independently rated on a scale of 1 to 5 using a validated instrument by two editors and the corresponding author. Authors and editors were blind to the intervention group. Authors rated review quality before the fate of their paper had been decided. Additional outcomes were the time taken to complete the review and the reviewer’s recommendation regarding publication.
Results 558 manuscripts were randomised, and 471 manuscripts remained after exclusions. Of the 1039 reviewers approached to take part in the study, 568 (55\%) declined. Two editors’ evaluations of the quality of the peer review were obtained for all 471 manuscripts, with the corresponding author’s evaluation obtained for 453. There was no significant difference in review quality between the intervention and control groups (mean difference for editors 0.04, 95\% CI −0.09 to 0.17; for authors 0.06, 95\% CI −0.09 to 0.20). Any possible difference in favour of the control group was well below the level regarded as editorially significant. Reviewers in the intervention group took significantly longer to review (mean difference 25 minutes, 95\% CI 3.0 to 47.0 minutes).
Conclusion Telling peer reviewers that their signed reviews might be available in the public domain on the BMJ’s website had no important effect on review quality. Although the possibility of posting reviews online was associated with a high refusal rate among potential peer reviewers and an increase in the amount of time taken to write a review, we believe that the ethical arguments in favour of open peer review more than outweigh these disadvantages.},
  journaltitle = {BMJ},
  shortjournal = {BMJ},
  urldate = {2019-10-25},
  date = {2010-11-16},
  pages = {c5729},
  author = {van Rooyen, Susan and Delamothe, Tony and Evans, Stephen J. W.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\4LP5VZPE\\Rooyen et al_2010_Effect on peer review of telling reviewers that their signed reviews might be.pdf},
  eprinttype = {pmid},
  eprint = {21081600}
}

@article{walshOpenPeerReview2000,
  langid = {english},
  title = {Open Peer Review: {{A}} Randomised Controlled Trial},
  volume = {176},
  issn = {0007-1250, 1472-1465},
  url = {https://www.cambridge.org/core/journals/the-british-journal-of-psychiatry/article/open-peer-review-a-randomised-controlled-trial/1F81447FC67B3BAFDCCCCE82B6C7A187},
  doi = {10.1192/bjp.176.1.47},
  shorttitle = {Open Peer Review},
  abstract = {Background
Most scientific journals practise anonymous peer review. There is no evidence, however, that this is any better than an open system.

Aims
To evaluate the feasibility of an open peer review system.

Method
Reviewers for the British Journal of Psychiatry were asked whether they would agree to have their name revealed to the authors whose papers they review; 408 manuscripts assigned to reviewers who agreed were randomised to signed or unsigned groups. We measured review quality, tone, recommendation for publication and time taken to complete each review.

Results
A total of 245 reviewers (76\%) agreed to sign. Signed reviews were of higher quality, were more courteous and took longer to complete than unsigned reviews. Reviewers who signed were more likely to recommend publication.

Conclusions
This study supports the feasibility of an open peer review system and identifies such a system's potential drawbacks.},
  number = {1},
  journaltitle = {The British Journal of Psychiatry},
  urldate = {2019-10-25},
  date = {2000-01},
  pages = {47-51},
  author = {Walsh, Elizabeth and Rooney, Maeve and Appleby, Louis and Wilkinson, Greg},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\LT9ETGKG\\Walsh et al_2000_Open peer review.pdf;C\:\\Users\\lissa102\\Zotero\\storage\\YZMR76SL\\1F81447FC67B3BAFDCCCCE82B6C7A187.html}
}

@article{ross-hellauerWhatOpenPeer2017,
  title = {What Is Open Peer Review? {{A}} Systematic Review},
  volume = {6},
  issn = {2046-1402},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5437951/},
  doi = {10.12688/f1000research.11369.2},
  shorttitle = {What Is Open Peer Review?},
  abstract = {Background: “Open peer review” (OPR), despite being a major pillar of Open Science, has neither a standardized definition nor an agreed schema of its features and implementations. The literature reflects this, with numerous overlapping and contradictory definitions. While for some the term refers to peer review where the identities of both author and reviewer are disclosed to each other, for others it signifies systems where reviewer reports are published alongside articles. For others it signifies both of these conditions, and for yet others it describes systems where not only “invited experts” are able to comment. For still others, it includes a variety of combinations of these and other novel methods., 
Methods: Recognising the absence of a consensus view on what open peer review is, this article undertakes a systematic review of definitions of “open peer review” or “open review”, to create a corpus of 122 definitions. These definitions are systematically analysed to build a coherent typology of the various innovations in peer review signified by the term, and hence provide the precise technical definition currently lacking., 
Results: This quantifiable data yields rich information on the range and extent of differing definitions over time and by broad subject area. Quantifying definitions in this way allows us to accurately portray exactly how ambiguously the phrase “open peer review” has been used thus far, for the literature offers 22 distinct configurations of seven traits, effectively meaning that there are 22 different definitions of OPR in the literature reviewed., 
Conclusions: I propose a pragmatic definition of open peer review as an umbrella term for a number of overlapping ways that peer review models can be adapted in line with the aims of Open Science, including making reviewer and author identities open, publishing review reports and enabling greater participation in the peer review process.},
  journaltitle = {F1000Research},
  shortjournal = {F1000Res},
  urldate = {2019-10-25},
  date = {2017-08-31},
  author = {Ross-Hellauer, Tony},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\IRD7VFHQ\\Ross-Hellauer_2017_What is open peer review.pdf},
  eprinttype = {pmid},
  eprint = {28580134},
  pmcid = {PMC5437951}
}

@online{TechBlogGitReproducibility,
  title = {{{TechBlog}}: {{Git}}: {{The}} Reproducibility Tool Scientists Love to Hate : {{Naturejobs Blog}}},
  url = {http://blogs.nature.com/naturejobs/2018/06/11/git-the-reproducibility-tool-scientists-love-to-hate/},
  urldate = {2019-10-26},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\UA5Y7BXZ\\git-the-reproducibility-tool-scientists-love-to-hate.html}
}

@article{blischakQuickIntroductionVersion2016,
  langid = {english},
  title = {A {{Quick Introduction}} to {{Version Control}} with {{Git}} and {{GitHub}}},
  volume = {12},
  issn = {1553-7358},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004668},
  doi = {10.1371/journal.pcbi.1004668},
  number = {1},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  urldate = {2019-10-26},
  date = {2016-01-19},
  pages = {e1004668},
  keywords = {Cloning,Control systems,Graphical user interface,Kidneys,Machine learning,Morphology (linguistics),Scientists,Software development},
  author = {Blischak, John D. and Davenport, Emily R. and Wilson, Greg},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\NYSUERQ3\\Blischak et al_2016_A Quick Introduction to Version Control with Git and GitHub.pdf;C\:\\Users\\lissa102\\Zotero\\storage\\BCILHZ8S\\article.html}
}

@online{SupplementalUsingGit,
  title = {Supplemental: {{Using Git}} from {{RStudio}} – {{Version Control}} with {{Git}}},
  url = {https://swcarpentry.github.io/git-novice/14-supplemental-rstudio/index.html},
  urldate = {2019-10-26},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\7XNF7PKP\\index.html}
}

@online{ReproducibleWorkflowVersion,
  title = {Reproducible Workflow and Version Control with {{Git}} and {{Github}}},
  url = {https://jules32.github.io/2016-07-12-Oxford/git/},
  urldate = {2019-10-26},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\7J672FN6\\git.html}
}

@article{stanleyReproducibleTablesPsychology2018,
  langid = {english},
  title = {Reproducible {{Tables}} in {{Psychology Using}} the {{apaTables Package}}},
  volume = {1},
  issn = {2515-2459},
  url = {https://doi.org/10.1177/2515245918773743},
  doi = {10.1177/2515245918773743},
  abstract = {Growing awareness of how susceptible research is to errors, coupled with well-documented replication failures, has caused psychological researchers to move toward open science and reproducible research. In this Tutorial, to facilitate reproducible psychological research, we present a tool that creates reproducible tables that follow the American Psychological Association’s (APA’s) style. Our tool, apaTables, automates the creation of APA-style tables for commonly used statistics and analyses in psychological research: correlations, multiple regressions (with and without blocks), standardized mean differences, N-way independent-groups analyses of variance (ANOVAs), within-subjects ANOVAs, and mixed-design ANOVAs. All tables are saved as Microsoft Word documents, so they can be readily incorporated into manuscripts without manual formatting or transcription of values.},
  number = {3},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  shortjournal = {Advances in Methods and Practices in Psychological Science},
  urldate = {2020-01-03},
  date = {2018-09-01},
  pages = {415-431},
  keywords = {replication,reproducibility,data sharing,open data,open materials,open science,R,reproducible analyses,reproducible research,reproducible tables,statistical tools,transparency in research},
  author = {Stanley, David J. and Spence, Jeffrey R.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\SYWRI2PD\\Stanley_Spence_2018_Reproducible Tables in Psychology Using the apaTables Package.pdf}
}

@article{ExpandingReachPsychological2019,
  langid = {english},
  title = {Expanding the {{Reach}} of {{Psychological Science}}},
  issn = {0956-7976},
  url = {https://doi.org/10.1177/0956797619898664},
  doi = {10.1177/0956797619898664},
  journaltitle = {Psychological Science},
  shortjournal = {Psychol Sci},
  urldate = {2020-01-08},
  date = {2019-12-18},
  pages = {0956797619898664},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\RIZC4MV4\\2019_Expanding the Reach of Psychological Science.pdf}
}

@article{aczelConsensusbasedTransparencyChecklist2019,
  langid = {english},
  title = {A Consensus-Based Transparency Checklist},
  issn = {2397-3374},
  url = {https://www.nature.com/articles/s41562-019-0772-6},
  doi = {10.1038/s41562-019-0772-6},
  abstract = {We present a consensus-based checklist to improve and document the transparency of research reports in social and behavioural research. An accompanying online application allows users to complete the form and generate a report that they can submit with their manuscript or post to a public repository.},
  journaltitle = {Nature Human Behaviour},
  shortjournal = {Nat Hum Behav},
  urldate = {2020-01-08},
  date = {2019-12-02},
  pages = {1-3},
  author = {Aczel, Balazs and Szaszi, Barnabas and Sarafoglou, Alexandra and Kekecs, Zoltan and Kucharský, Šimon and Benjamin, Daniel and Chambers, Christopher D. and Fisher, Agneta and Gelman, Andrew and Gernsbacher, Morton A. and Ioannidis, John P. and Johnson, Eric and Jonas, Kai and Kousta, Stavroula and Lilienfeld, Scott O. and Lindsay, D. Stephen and Morey, Candice C. and Munafò, Marcus and Newell, Benjamin R. and Pashler, Harold and Shanks, David R. and Simons, Daniel J. and Wicherts, Jelte M. and Albarracin, Dolores and Anderson, Nicole D. and Antonakis, John and Arkes, Hal R. and Back, Mitja D. and Banks, George C. and Beevers, Christopher and Bennett, Andrew A. and Bleidorn, Wiebke and Boyer, Ty W. and Cacciari, Cristina and Carter, Alice S. and Cesario, Joseph and Clifton, Charles and Conroy, Ronán M. and Cortese, Mike and Cosci, Fiammetta and Cowan, Nelson and Crawford, Jarret and Crone, Eveline A. and Curtin, John and Engle, Randall and Farrell, Simon and Fearon, Pasco and Fichman, Mark and Frankenhuis, Willem and Freund, Alexandra M. and Gaskell, M. Gareth and Giner-Sorolla, Roger and Green, Don P. and Greene, Robert L. and Harlow, Lisa L. and de la Guardia, Fernando Hoces and Isaacowitz, Derek and Kolodner, Janet and Lieberman, Debra and Logan, Gordon D. and Mendes, Wendy B. and Moersdorf, Lea and Nyhan, Brendan and Pollack, Jeffrey and Sullivan, Christopher and Vazire, Simine and Wagenmakers, Eric-Jan},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\5T92PG4Y\\Aczel et al_2019_A consensus-based transparency checklist.pdf;C\:\\Users\\lissa102\\Zotero\\storage\\ZLCD57B9\\s41562-019-0772-6.html}
}

@article{aalbersbergMakingScienceTransparent2018,
  title = {Making {{Science Transparent By Default}}; {{Introducing}} the {{TOP Statement}}},
  url = {https://osf.io/sm78t},
  doi = {10.31219/osf.io/sm78t},
  abstract = {In order to increase the replicability of scientific work, the scientific community has called for practices designed to increase the transparency of research (McNutt, 2014; Nosek et al., 2015). The validity of a scientific claim depends not on the reputation of those making the claim, the venue in which the claim is made, or the novelty of the result, but rather on the empirical evidence provided by the underlying data and methods. Proper evaluation of  the merits of scientific findings requires availability of the methods, materials, and data and the reasoned argument that serve as the basis for the published conclusions (Claerbout and Karrenbach 1992; Donoho et al 2009; Stodden et al 2013; Borwein et al 2013; Munafò et al, 2017). Wide and growing support for these principles (see, for example, signatories to Declaration on Research Assessment, DORA, https://sfdora.org/, and the Transparency and Openness Promotion Guidelines https://cos.io/our-services/top-guidelines/) must be coupled with guidelines to increase open sharing of data and research materials, use of reporting guidelines, preregistration, and replication. We propose that, going forward, authors of all scientific articles disclose the availability and location of all research items, including data, materials, and code, related to their published articles in what we will refer to as a TOP Statement.},
  urldate = {2020-01-08},
  date = {2018-02-15},
  author = {Aalbersberg, IJsbrand Jan and Appleyard, Tom and Brookhart, Sarah and Carpenter, Todd and Clarke, Michael and Curry, Stephen and Dahl, Josh and DeHaven, Alexander Carl and Eich, Eric and Franko, Maryrose and Freedman, Len and Graf, Chris and Grant, Sean and Hanson, Brooks and Joseph, Heather and Kiermer, Veronique and Kramer, Bianca and Kraut, Alan and Karn, Roshan Kumar and Lee, Carole and MacFarlane, Aki and Martone, Maryann and Mayo-Wilson, Evan and McNutt, Marcia and McPhail, Meredith and Mellor, David Thomas and Moher, David and Mudditt, Alison and Nosek, Brian A. and Orland, Belinda and Parker, Timothy H. and Parsons, Mark and Patterson, Mark and Santos, Solange and Shore, Carolyn and Simons, Daniel J. and Spellman, Bobbie and Spies, Jeffrey Robert and Spitzer, Matthew and Stodden, Victoria and Swaminathan, Sowmya and Sweet, Deborah and Tsui, Anne and Vazire, Simine}
}

@article{ramGitCanFacilitate2013,
  title = {Git Can Facilitate Greater Reproducibility and Increased Transparency in Science},
  volume = {8},
  issn = {1751-0473},
  url = {https://doi.org/10.1186/1751-0473-8-7},
  doi = {10.1186/1751-0473-8-7},
  abstract = {Reproducibility is the hallmark of good science. Maintaining a high degree of transparency in scientific reporting is essential not just for gaining trust and credibility within the scientific community but also for facilitating the development of new ideas. Sharing data and computer code associated with publications is becoming increasingly common, motivated partly in response to data deposition requirements from journals and mandates from funders. Despite this increase in transparency, it is still difficult to reproduce or build upon the findings of most scientific publications without access to a more complete workflow.},
  number = {1},
  journaltitle = {Source Code for Biology and Medicine},
  shortjournal = {Source Code for Biology and Medicine},
  urldate = {2020-01-08},
  date = {2013-02-28},
  pages = {7},
  author = {Ram, Karthik},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\RZB63RDE\\Ram_2013_Git can facilitate greater reproducibility and increased transparency in science.pdf;C\:\\Users\\lissa102\\Zotero\\storage\\H4UH8INM\\1751-0473-8-7.html}
}

@online{EditorsPsychOpen,
  title = {For {{Editors}}: {{PsychOpen}}},
  url = {https://www.psychopen.eu/for-editors/},
  urldate = {2020-01-08}
}

@article{moreauMetaanalysisTemplatesMaterials2019,
  langid = {english},
  title = {Meta-Analysis Templates and Materials},
  url = {https://osf.io/q8stz/},
  doi = {None},
  abstract = {Hosted on the Open Science Framework},
  urldate = {2020-01-08},
  date = {2019-11-28},
  author = {Moreau, David and Gamble, Beau},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\ZI3WLI64\\q8stz.html}
}

@online{muenchenPopularityDataScience2012,
  langid = {american},
  title = {The {{Popularity}} of {{Data Science Software}}},
  url = {http://r4stats.com/articles/popularity/},
  abstract = {by Abstract This article, formerly known as The Popularity of Data Analysis Software, presents various ways of measuring the popularity or market share of software for advanced a…},
  journaltitle = {r4stats.com},
  urldate = {2020-01-08},
  date = {2012-04-25T23:22:32+00:00},
  author = {Muenchen, Robert A.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\WEGIDWCV\\popularity.html}
}

