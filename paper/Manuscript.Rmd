---
title             : "WORCS: A Workflow for Open Reproducible Code in Science"
shorttitle        : "WORKFLOW FOR OPEN SCIENCE"

author: 
  - name          : "Caspar J. van Lissa"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Padualaan 14, 3584CH Utrecht, The Netherlands"
    email         : "c.j.vanlissa@uu.nl"

affiliation:
  - id            : "1"
    institution   : "Utrecht University faculty of Social and Behavioral Sciences, department of Methodology & Statistics"

abstract: |
  Abstract here
  
keywords          : "open science; reproducibility; r; dynamic document generation; version control"
wordcount         : "7356"

bibliography      : ["worcs.bib"]  

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : yes
mask              : yes

class             : "man"
classoption       : "noextraspace"
output            : papaja::apa6_pdf
urlcolor          : blue
---

```{r load_packages, include = FALSE}
library("papaja")
library(motley)
#source("rfunctions.R")
#load("results.RData")
#load("table_coefficients.RData")
mask <- TRUE
masked <- function(text){
  ifelse(mask, "[masked]", text)
}
```

```{r analysis_preferences}
# Seed for random number generation
set.seed(42)
```

\setlength{\parskip}{0em}

\vspace{\baselineskip}

<!-- Structure
# Introduction
# Defining Open Science
* TOP guidelines
# Goal of this paper
* Grass-roots
* Open science as opportunity ?move
    - Easier work
    - Less mistakes
    - Better collaboration
    - Cumulative science
    - Error correction
# Introducing the tools
# Setup - do this only once
# WORCS - steps to follow for each project
# Discussion
* advantages
* limitations
* future directions
-->

# Introduction

The social sciences are amidst a paradigm shift towards open science. Several immediate causes stirred up support for this transition; including several highly publicized cases of scientific fraud, increasing awareness of questionable research practices, and the replication crisis. However, Open science is not merely a cure for this crisis - it is also an opportunity. Technological advances enable researchers to more easily conduct reliable, cumulative, collaborative science. Capitalizing on these advances has the potential to accellerate scientific progress.

This paper introduces WORCS: A Workflow for Open Reproducible Code in Science. WORCS is a lightweight approach to open science and computational reproducibility. It provides a step-by-step procedure that researchers can follow to make an entire research project Open and Reproducible. WORCS is based on best practices, and can be used either in parallel to, or in absence of, top-down requirements by journals. The main goal of WORCS is to lower the threshold for adopting an Open Science workflow by providing the present tutorial, along with an R-package with user-friendly support functions, and an Rstudio project template. 

<!--
thereby lowering the threshold for adoption.
, and still ensures reproducibility in most circumstances. 
 It is easy to implement, thereby lowering the threshold 

* Grass-roots
* Open science as opportunity
    - Easier work
    - Less mistakes
    - Better collaboration
    - Cumulative science
    - Error correction

Benefits to individual scientists: Automation (after a learning curve), thus more productive, avoid mistakes, join projects, clone projects and make them your own.
Collective benefits: Cumulative science, -->

# Defining Open Science

One of the most impactful contributions to Open Science has been the development of the TOP-guidelines, and the effort to urge publishers to adopt them. These guidelines describe eight standards for Open Science: 1) Comprehensive citation of literature, data, materials, and methods; sharing 2) data, 3) code required to reproduce analyses, 4) new research materials, and 5) details of the design and analysis; 6) pre-registration of studies before data collection, and 7) pre-registration of the analysis plan prior to analysis; and 8) replication of published results. WORCS defines the goals of Open Science in terms of these guidelines, and is designed to facilitate meeting each of these guidelines, with one exception: We do not address replication of published results, because replication relates to the topic of a research project, instead of its execution. We group the remaining seven guidelines into three categories: Citation (1), sharing (2-5), and preregistration (6-7).

## Goal of this paper

The TOP-guidelines are primarily geared towards top-down systemic change (what's in a name) by Journals, funders, and organizations. Yet there is also substantial momentum for grassroots change. Many individual researchers are willing to adopt best practices for Open Science, even in the absence of institutional support. The formation of "Open Science Communities" at universities around the globe illustrates this movement. This paper is designed as an instrument for grassroots change, that can help individual scientists to meet the principles of Open Science. This workflow can be used regardless of any top-down Open Science requirements already implemented by journals and institutions: If such requirements exist, this workflow will help fulfill them, and if they do not exist, facilitates meeting them.

## Existing solutions

There have been several previous efforts to promote grass-roots adoption of Open Science principles. Each of these efforts has a different scope, strengths, and limitations that set it apart from WORCS. For instance, @aalbersbergMakingScienceTransparent2018 suggested publishing a "TOP-statement" as supplemental material, which discloses the authors' adherence to Open Science principles. This is very easy to implement, and addresses TOP-guidelines 1-7. A limitation of this approach, which WORCS addresses, is that it does not clarify *how* researchers can meet the TOP-guidelines. One could use WORCS to meet the TOP-guidelines, and document this in a TOP-statement. A second relevant source is @peikertReproducibleDataAnalysis2019, who describe a workflow for reproducible analyses. Being limited to computational reproducibility, this workflow only addresses TOP-guidelines 2, 3, and 5. Peikert and Brandmaier aim to achieve strict computational reproducibility by conducting analyses on a virtual machine, that can be stored like a sort of time capsule and reinstated on different systems, instead of on a private computer. The workflow is excellent, but difficult to implement for non-programmers. WORCS instead uses a much more lightweight approach to safeguard computational reproducibility under *most circumstances*. This should be sufficient for most users, but can also be combined with Peikert and Brandmaier if strict computational reproducibility is essential. WORCS thus addresses a unique issue not covered by existing solutions, namely to provide a workflow most conducive to satisfying the TOP-guidelines. Still, it can be combined with existing approaches.

```{r, echo = FALSE, eval = FALSE}
tab1 <- data.frame(
  Reference = c("Aalbers et al., 2018", "Peikert & Brandmaier, 2019"),
  Scope = c("Disclose adherence to Open Science principles", "Completely reproducible analysis"),
  Difficulty = c("Easy", "Hard"),
  Effectiveness = c("Low", "High"),
  "TOP-guidelines" = c("1-7", "2,3,5")
)
apa_table(tab1)
```

## Introducing the tools

WORCS relies on a few free, open source software solutions that, when combined, facilitate meeting the TOP-guidelines related to citation, sharing, and preregistration. The first tool is *dynamic document generation*. Dynamic document generation (DDG) means writing scientific reports in a format that interleaves written reports with blocks of computer code used to conduct the analyses. The text is automatically formatted as an APA-style manuscript, and results of the code blocks are automatically generated and insterted in the text, or rendered as Figures and Tables. Dynamic document generation supercedes the classical approach of using separate programs to write prose and conduct analyses, and then manually copy-pasting analysis results into the text. Although there is a slight learning curve to transitioning to DDG, the investment really pays off: First, the time saved from painstakingly copy-pasting output and manually formatting text soon outweighs the investment of switching to a new program; second, human error in copy-pasting results is eliminated; third, when revisions require major changes to the analyses, all results, Figures and Tables are automatically updated; and finally, flexibility in output formats means that a manuscript can be rendered to presentation format, or even to a website. In sum, while writing academic papers in a programming environment might seem counter-intuitive at first, this approach is much more amenable to the needs of academics than most word processing software. It prevents mistakes, and saves time.

The second tool is *version control*: Maintaining an indelible log of every change to all project files. Version control is a near-essential tool for scientific reproducibility, as anyone learns who has had the experience of accidentally deleting a crucial file, or of being unable to reproduce analyses because they ran some analyses interactively instead of using syntax. Many scientists use some form of *implicit* version control; for example, by renaming files after major changes (e.g., "manuscript_final_2.2-2019-10-12.doc"), tracking changes in word processing software, or using cloud hosting services that retain backups of previous versions. WORCS instead uses the *explicit* version control software "Git". Git tracks changes to files, and stores these changes when the user makes a "commit". Git retains a complete log of all commits, and users can compare changes between different commits, or go back to a previous version of the code (for example, after making a mistake, or to replicate a previous version of the results). A project version controlled with Git is called a "Repository", or Repo.

<!--In statistical consultations, I often hear that researchers are unable to reproduce specific results because they only worked with a "very specific" version of the syntax - usually a combination of -->

The functionality of Git is amplified by services such as GitHub (other services are available). GitHub is best understood as a cloud storage service with social networking functionality. The "cloud storage" aspect of GitHub works as follows: You can "clone" a local Git repository to the GitHub website, as a backup or research archive. The "social network" aspect comes into play when the project is set to "public": This allows other researchers to peruse the repository on the GitHub website to see how the work was done, clone it to their own computer to replicate the original work or apply the methods to their own data, open Issues to ask questions or give feedback on the project, or even send a "Pull request" with changes to the text or code for your consideration. Git and GitHub shine as tools for collaboration, because different people can simultaneously work on different parts of a project, and their changes can be compared and automatically merged on the website. Even on solo projects, working with Git/GitHub has many benefits: Staying organized, being able to start a new study with a clone of an old, similar repository, or splitting off an "experimental branch" to try something new, while retaining the ability to revert to a previous state of the project, or to merge the experimental branch.

The third tool is *dependency management*: Keeping track of exactly what software was used to conduct the analyses. <!-- Being able to replicate analyses requires not only access to the data and analysis code, but also to the software used to evaluate that code.--> At first glance, it might seem sufficient to state that analyses were conducted in *Program X*. However, every program is susceptible to changes and updates. R-packages in particular are updated frequently, because they are Open Source, and there is an active community of developers contributing functionality and bugfixes. Potentially, any update could change the results of the code. For example, R-version 3.6.0 fixed a bug in the random number generator. This means that any analyses that use random numbers, such as bootstrapping, cannot be strictly replicated from previous versions.

Many solutions exist to ensure computational reproducibility. These solutions typically work by enveloping your research project in a distinct "environment" that only has access to programs that are explicitly installed, and maintaining a record of these programs. These solutions differ in user-friendliness and effectiveness. If strict reproducibility is required, one might use the aforementioned workflow by @peikertReproducibleDataAnalysis2019, which runs all analyses inside a virtual machine. This is quite effective, but very difficult to set up. A slightly less sophisticated solution is offered by the R-package `packrat`, which installs all software used in a project to a dedicated library that can be stored and copied with the project. This solution is reasonably user-friendly and effective, but has the disadvantage that for each project, all packages must be installed anew, which can take a long time (in my case, hours) and a lot of hard drive space. Moreover, Git/GitHub is not designed to store your entire project library, so `packrat` does not address the issue of how other users are supposed to get access to your library. In conclusion, `packrat` is somewhat unwieldy. The package `checkpoint` offers an extremely lightweight solution: Instead of storing all software used, it simply records the calendar date, and then installs the version of all required packages that was available on the central R repository `CRAN` on that date from an archival website. Thus, all that is needed to restore a project is the date on which it was conducted. This is very user-friendly, and reasonably effective - the only problem is that this solution is limited to `CRAN` packages, and cannot mix package versions from different dates.

A recently developed package strikes a great balance between user-friendliness and effectiveness: `renv`, developed by the team behind Rstudio. This package maintains a text-based log file of which packages are used, in what versions, and where they were installed from (e.g., CRAN, Bioconductor, GitHub). Being text-based, this log can be version controlled with Git. On a local computer, `renv` installs all packages used in a cache that is shared between all projects that use `renv`. Thus, if one of your projects uses a version of a package that is already in the cache, it does not need to be installed again. This overcomes the limitations of long installation times and large space requirements of `renv`. When someone else loads your project, `renv` will install all of the required packages onto a cache on their computer. Whereas `packrat` copies the source code of all packages, `renv` obtains the packages from their original repositories. Thus, as long as these repositories used maintain their archives, this solution is very lightweight and effective. It is not foolproof, but some have argued that all solutions for computational reproducibility have a limited shelf life [@brownHowLearnedStop2017]. Ultimately, the best solution might be to use a good-enough solution, like `renv`, and acknowledge that all code requires some maintenance if you want to reproduce it in the future.

## Additional considerations

A few additional considerations were considered in developing WORCS. The first consideration is to develop a workflow for R, instead of for SPSS or Python. Several arguments support the choice to work in R. First, R and all of it extensions are free and Open Source, and Open Source software should be the tool of choice for Open Science. Second, all of the aforementioned tools for an Open Science workflow are implemented in R. In general, many statistical methods are first implemented in R, as there is a vibrant online community developing new R-packages and providing support and tutorials for existing ones. Third, at the time of writing, R is the second-most cited statistical software package [@PopularityDataScience2012], following SPSS, which has no support for any of the tools discussed in this paper. Finally, R is highly interoperable: Packages are available to load nearly every imaginable filetype, code written in other programming languages [including Python, @allaireMarkdownPythonEngine] can be called from R, and output can be written to many file types, including DOCX and PDF (in APA style), and HTML format. 

Working with R is simplified immensely by using the Rstudio user interface for R. Rstudio integrates many of the tools used in WORCS in a user-friendly manner, and automates and streamlines many tedious or complicated aspects of working with R. This renders Rstudio a catch-all solution for generating open, reproducible code. One important feature of Rstudio is project management. A project bundles writing, analyses, data, references, etcetera, into a self-contained unit that can be uploaded to GitHub and downloaded by future users. The `worcs` R-package installs a new Rstudio project template, that performs much of the bookkeeping required to set up an Open Science project automatically  behind the scenes.

Another key consideration is what filetypes to use. A case can be made to use text-based files only (or as much as possible), instead of binary files. Text-based files can be read by machines and humans alike, without requiring special (licensed) software. Binary files must be decoded by software first. Git is designed to keep track of changes to the content of text-based files. Each change reflects a word or line of code added or removed. Binary files such as `.docx` or `.sav`, on the other hand, should not be version controlled. The changes in the encrypted file are uninterpretable. Moreover, text-based files are often smaller than binary files, which means they take up less space on cloud hosting services, such as GitHub. Uploading large binary files is frowned upon. GitHub also allows visitors to peruse text-based files online, and conveniently renders certain filetypes. For example, GitHub highlights syntax in `.R` and `.Rmd` files, and displays `.csv` files as spreadsheets.

<!--

- Use RStudio projects to bundle analyses and writing into a self-contained unit
- Use text-based files only
    * `.R` for analyses
    * `.Rmd` for documents
    * `.csv` for data
- Version control all these files with Git
- Ensure full reproducibility of the R environment with the `renv` package, which takes a snapshot of all packages used (and their version numbers), and can restore this configuration on a new system
- Publish the entire self-contained research unit on GitHub
- Connect this GitHub repository to an OSF project
-->

## Open science as opportunity

Sometimes, Open Science principles are discussed as a kind of "antidote" to scientific misconduct. Yet these principles also present an opportunity to conduct more reliable research, with fewer mistakes, more convenient collaboration, more effective internal and external collaboration, and the potential for crowd-sourced error correction. 


## How WORCS meets the goals of Open Science

### Comprehensive citation

TOP encourages comprehensive citation of literature, data, materials, methods, and software. In principle, researchers can meet this requirement by simply citing every reference used. Unfortunately, citation of data and software is less commonplace than citation of literature and materials. Crediting these resources is important, because it incentivizes data sharing and the development of open-source software, and supports the open science efforts of others. 

<!--Various initiatives are underway to innovate how scientific output is measured and valued. https://www.nwo.nl/beleid/statement+waarderen+en+belonen+van+wetenschappers --> 
To facilitate citing datasets, researchers sometimes publish *data papers*; documents that detail the procedure, sample, and codebook. Specialized journals, such as the [*Journal of Open Psychology Data*](https://openpsychologydata.metajnl.com/), aid in the publication of these data papers. For smaller projects, researchers sometimes publish their data in an online repository - like GitHub - along with a text file with the preferred citation for the data (which can be a substantive paper), and the license that applies to the data, such as Creative Commons [BY-SA](https://creativecommons.org/licenses/by-sa/4.0/) or [BY-NC-SA](https://creativecommons.org/licenses/by-nc-sa/4.0/). When in doubt, one can always contact the data creators and ask what the preferred citation is. 

When using R, software (R packages) can be conveniently cited by calling `citation("packagename")`. This returns an APA-style reference, and a BibTeX entry. However, users of the WORCS project template for Rstudio do not need to manually cite R-packages. WORCS generates a template manuscript in APA6 style, based on the `papaja` package [@austPapajaPrepareReproducible2020], which automatically cites all packages used.

One important impediment to comprehensive citation is the fact that print journals operate with space constraints. Print journals often discourage comprehensive citation, either actively, or passively by including the reference list in the manuscript word count. Researchers can overcome this impediment by preparing two versions of the manuscript: One version with comprehensive citations for online dissemination, and another version for print, with only the essential citations. The print version should reference the online version in the print version, so interested readers can find the comprehensive reference list. Step 20 of the WORCS procedure suggests uploading the online version to a preprint server. This is important because most preprint servers - [at least the ones hosted by OSF.io](https://help.osf.io/hc/en-us/articles/360019930493-Preprint-FAQs#how_do_i_find_preprints) - are indexed by Google Scholar. This means that authors will receive credit for cited work; even if they are cited only in the online version. Moreover, preprint servers ensure that the online version will have a persistent DOI, and will remain reliably accessible, just like the print version. 

It is easiest to mark the distinction between essential and non-essential references from the start, instead of going back to cut references prior to publication. Therefore, the `worcs` package distinguishes between the traditional at-symbol (`@`) used to cite a reference, and the "double at"-symbol (`@@`), used to cite a non-essential reference. Users can render a manuscript to PDF either with, or without, comprehensive citations.

The procedure for citation in WORCS is as follows:

1. During writing, maintain a plain-text `.bib` file with the BibTeX references for all citations.
  + You can make this file by hand; e.g., Figure \@ref(fig:scholarbib) shows how to obtain a BibTeX reference from Google Scholar; simply copy-paste each reference into the `.bib` file
  + You can export a `.bib` file from most reference manager programs; the free, open-source reference manager [Zotero](https://www.zotero.org/download/) is excellent and user-friendly, and highly interoperable with other commercial reference managers. [Here](https://christopherjunk.netlify.com/blog/2019/02/25/zotero-rmarkdown/) is a tutorial for using Zotero with Rmarkdown.
2. To cite a reference, use the `citekey` - the first word in the BibTeX entry for that reference. Insert it in the Rmarkdown file like so: `@yourcitekey2020`. For a parenthesized reference, use `[@citekeyone2020; @citekeytwo2020]`. For more options, see the [Rmarkdown cookbook](https://bookdown.org/yihui/rmarkdown-cookbook/bibliography.html).
3. To indicate a *non-essential* citation, mark it with a double at-symbol: `@@nonessential2020`.
4. When Knitting the document, adapt the `knit` command in the YAML header.  
  `knit: worcs::cite_all` renders all citations, and  
  `knit: worcs::cite_essential` removes all *non-essential* citations.
5. Optional: To be extremely thorough, you could make a "branch" of the Github repository for the print version of the manuscript. Only in this branch, you use the function `knit: worcs::cite_essential`. The procedure is documented in [this tutorial](http://rstudio-pubs-static.s3.amazonaws.com/142364_3b344a38149b465c8ebc9a8cd2eee3aa.html).

```{r scholarbib, echo = FALSE}
knitr::include_graphics("scholar_bib.png")
```

<!--
Future: Explain why each reference is cited? How best to include this?
* Note
* In a function
* In a separate document-->

### Data sharing

Data sharing is important for computational reproducibility and secondary analysis. Computational reproducibility means that a third party can exactly recreate the results from the raw data, using the published analysis code. Secondary analysis means that a third party can conduct sensitivity analyses, explore alternative explanations, or even use existing data to answer a different research question. From an open science perspective, data sharing is always desirable. From a practical point of view, it is not always possible. Data sharing may be impeded by legal constraints, ethical restrictions, absence of informed consent for data sharing, and privacy concerns. In some cases, it is difficult or impossible to share data.

When data can be shared, it is important to version control the (anonymized/pseudonymized) raw data upon collection, and avoid changing it. Any data processing steps should be documented in the code, instead of applied to the raw data. This way, the code will be a pipeline from the raw data to the published results.

When data cannot be shared, researchers should aim to safeguard the potential for computational reproducibility and secondary analysis as much as possible. WORCS uses two solutions to accomplish this goal. The first solution is publishing a *checksum* of the raw data upon collection. A checksum is a 32-character summary of a file of any size. Any change to the underlying file results in a different checksum. Thus, one can use a checksum to verify that the contents of a file remain unchanged. If researchers publish the checksum of their raw data and the complete analysis code, then they can prove to an independent auditer that running the public analysis code on the private dataset with the correct checksum results in the published results of their work.

The second solution is to share a synthetic dataset with similar characteristics to the real data. Synthetic data mimic the level of measurement and (conditional) distributions of the real data. Sharing synthetic data allows any third party to 1) verify that the published code runs, 2) debug the code, and 3) write valid code for alternative analyses. However, it is important to know that complex multivariate relationships present in the real data are often lost in synthetic data. Thus, findings from the real data might not be replicated in the synthetic data, and findings in the synthetic data *should not be substantively interpreted*. Still, sharing synthetic data facilitates data requests from third parties. A third party can write analysis code based on the synthetic data, and send it to the authors who evaluate it on the real data and send back the results. The ["Pull request"](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/creating-a-pull-request) functionality on GitHub is a particularly user-friendly way to do this.

The procedure for data sharing in WORCS is as follows. First, researchers load the data into R. This process should be documented in the file `prepare_data.R`, which is automatically created by the WORCS-template. This file is intended to document any steps that must be taken to get your data *into* R, and that are not intended to be repeated when re-running the analyses. Use this file to report any preprocessing required to obtain a neat `data.frame`. For example, if the data was originally in SPSS format, this file might just contain the code required to read the SPSS file. 

Then, researchers must decide whether to use open or closed data, and call one of these convenience functions:

* `open_data(data)`: This function takes a data object (a `data.frame` or `matrix`), writes it to a file called `data.csv`, updates the `.gitignore` file to stop ignoring this specific `.csv` file, and writes a checksum for this data file to `checksums.csv`. The data can now be committed and pushed to GitHub, where they will be open once the GitHub repository is made public.
* `closed_data(data)`: This function generates a local file called `data.csv`, and a synthetic dataset called `synthetic_data.csv`, updates the `.gitignore` file to stop ignoring `synthetic_data.csv` file, and writes a checksum for the real data file to `checksums.csv`. The synthetic data can now be committed and pushed to GitHub, and the real data will be ignored by Git.

Typically, the last line of the `prepare_data.R` file will thus be `open_data()` or `closed_data()`. The code for analysis of the data, on the other hand, will typically begin with `load_data()`. This function loads the real data if it is available (i.e., the file `data.csv` exists, and its checksum is correct), and otherwise, loads the synthetic data. This will have the effect that third party users who copy the GitHub repository will automatically load the synthetic dataset, whereas the study authors, who have the real data stored locally, will automatically load the real data.

### Sharing code, research materials, design and analysis

When writing a manuscript in Rmarkdown, analysis code is embedded in the prose of the paper. Thus, the TOP-guideline of sharing analysis code can be met simply by committing the source code of the manuscript to GitHub, and making this GitHub repository Public upon publication. If authors additionally use open data and a reproducible environment (default in WORCS), then a third party can simply replicate all analyses by copying the entire GitHub repository, and Knitting the manuscript on their local computer.

Aside from analysis code, the TOP-guidelines also encourage sharing new research materials, and details of the study design and analysis. These goals can be accomplished simply by placing any relevant documents in the Rstudio project folder, committing them to the Git repository, and pushing to GitHub. As with any document version controlled in this way, it is advisable to use plain text only.

### Preregistration

Lindsay [-@lindsayResearchPreregistration1012016] defines preregistration as "creating a permanent record of your study plans before you look at the data. The plan is stored in a date-stamped, uneditable file in a secure online archive." Two such archives are well-known in the social sciences: AsPredicted.org, and OSF.io. However, GitHub also conforms to these standards. Thus, it is possible to preregister a study simply by pushing a text document with the study plans to GitHub. If the GitHub repository is Public, the preregistration is immediately visible to the world, and Reviewers can submit comments through pull requests. If the repository is Private, Reviewers can be invited as "Collaborators", although this removes their anonymity. Private repositories can be made public at a later date, along with their entire time-stamped history. 

<!--Which studies can be preregistered is a matter of some debate. Some have taken the position that studies can only be preregistered prior to data collection. This is the only way to ensure that a researcher is not "HARKing": Hypothesizing after the results are known. From this point of view, it makes little -->

WORCS uses the following approach to preregistration:

1. Document study plans in a `preregistration.Rmd` file, and planned analyses in a `.R` file.
2. Commit these documents to the local Git repository, and push them to GitHub
3. On GitHub, [tag the release](https://help.github.com/en/github/administering-a-repository/creating-releases) as "Preregistration". A tagged release helps others retrieve this commit.
4. Optional: Upload the files as attachments to another pre-registration repository, e.g., AsPredicted.org or OSF.io.
  + The `.Rmd` file can be Knitted to a PDF form before uploading to a repository


#### Preregistering study plans

Several excellent preregistration templates are available in the R-package `prereg` [@austPreregMarkdownTemplates2019], including templates from AsPredicted.org, OSF.io, and from individual researchers [e.g., @vantveerPreregistrationSocialPsychology2016]. When opening a new Rstudio project with the WORCS project template, it is possible to select one of these preregistration templates, which will generate a file called `preregistration.Rmd`. This file should be used to document study plans, ideally prior to data collection.

#### Preregistering analyses

The ideal preregistered analysis consists of a complete analysis script that can be evaluated once the data are obtained. This ideal is often unattainable, because the data present researchers with unanticipated challenges; e.g., assumptions are violated, or analyses work differently than expected. Some of these challenges can be avoided by simulating the data one expects to obtain, and writing the analysis syntax based on the simulated data. This topic is beyond the scope of the present paper, but many user-friendly methods for simulating data are available in `R`, including the package [`simstudy`](https://cran.r-project.org/web/packages/simstudy/vignettes/simstudy.html). When using the WORCS-workflow, it is recommended to preregister, if possible, a `.R` script with the planned analyses, along with a verbal, conceptual description. If any changes must be made to the R-script after obtaining the data, one can refer to the conceptual description to explain why these changes are necessary.


<!--
3. Write conceptual preregistration
* https://osf.io/zab38/wiki/home/ Templates of OSF Registration Forms
* R package prereg
* mention other examples and cite van 't Veer, A. E., & Giner-Sorolla, R. (2016). Pre-registration in social psychology---A discussion and suggested template. Journal of Experimental Social Psychology, 67, 2--12. doi: 10.1016/j.jesp.2016.03.004
3. Commit, push, and tag th as preregistration
Preregistration: tag on github
https://stackoverflow.com/questions/18216991/create-a-tag-in-a-github-repository

4. Optional: Add Materials to repository. It is possible to solicit feedback (through issue/pull request) and later acknowledge outside contributions
4. Collect data and either add to repository, or register hash of raw datafile
5. Conduct analyses and write; commit regularly (ideally, after completing a small and clearly defined task), with informative commit messages. Call renv::snapshot() to save the state of the project library to the lockfile (called renv.lock),
6. Create OSF project, connect to GitHub, mention OSF page in manuscript
7. Optional: Publish preprint in a not-for-profit preprint repository (check Sherpa Romeo) http://sherpa.ac.uk/romeo/index.php
7. Submit
8. Make repository public -->


# Workflow for Open Reproducible Code in Science

## Preparing your system

Before you can use the WORCS workflow, you have to install the required software, and register for a GitHub account. The setup process is documented in a vignette included with the `worcs` R-package, and also [available online](https://cjvanlissa.github.io/worcs/articles/setup.html).

## Phase 1: Study design

1. Create a new Private repository on github, copy the https:// link to clipboard  
  The link should look something like https://github.com/yourname/yourrepo.git
2. In Rstudio, click File > New Project > New directory > WORCS Project Template
    a. Paste the GitHub Repository address in the textbox
    b. Keep the checkbox for `renv` checked if you want to document all dependencies (recommended). The only reason not to do this is if you have dependencies outside of the R environment, or if you are using a more comprehensive solution for computational reproducibility, e.g., @peikertReproducibleDataAnalysis2019.
    c. Select a preregistration template
    d. Click Open in new session, and Create project
3. Write the preregistration `.Rmd`
4. In the top-right corner of Rstudio, select the Git tab, select the checkboxes next to all files, and click the Commit button. Write an informative message for the commit, e.g., "Preregistration", again click Commit, and then click the green Push arrow to send your commit to GitHub
5. Go to the GitHub repository for this project, and tag the Commit as a preregistration
6. Optional: Render the preregistration to PDF, and upload it to AsPredicted.org or OSF.io as an attachment
7. Optional: Add study Materials (to which you own the rights) to the repository. It is possible to solicit feedback (by opening a GitHub Issue) and acknowledge outside contributions (by accepting Pull requests)

## Phase 2: Data analysis

8. Read the data into R, and document this procedure in `prepare_data.R`
9. Use `open_data()` or `closed_data()` to store the data
10. Write the manuscript in `Manuscript.Rmd`, using code chunks to perform the analyses.
11. Regularly commit your progress to the Git repository; ideally, after completing each small and clearly defined task. Use informative commit messages. Push the commits to GitHub.
12. Cite essential references with one at-symbol (`[@essentialref2020]`), and non-essential references with a double at-symbol (`[@@nonessential2020]`).

## Phase 3: Paper submission

13. Save the state of the project library (all packages used), call `renv::snapshot()`. This updates the lockfile, `renv.lock`.
14. Add an Open Science statement to the manuscript, which references the online repository for the project. This reference can be masked for blind review. The Open Science statement should indicate whether additional references are available in the online repository; whether data, code, materials, and study design details are shared, and whether a pre-registration is available. For further guidance, see @aalbersbergMakingScienceTransparent2018. Example:  
    > In the spirit of Open Science, an online repository is available at XXX, which contains more comprehensive citations, [the data/a synthetic data file], analysis code, the research materials used, details about the study design, and a tagged commit with a preregistration.

15. To render the paper with essential citations only for submission, change the line `knit: worcs::cite_all` to `knit: worcs::cite_essential`. Then, press the Knit button to generate a PDF

## Phase 4: Publication

Note that some researchers take these steps at an earlier stage. However, for newcomers to Open Science, who are unsure about potential implications for the publication of their work, it might be prudent to perform these actions after acceptance, and before publication:

16. Make the GitHub repository public
17. [Create an OSF project](https://help.osf.io/hc/en-us/articles/360019737594-Create-a-Project); although you may have already done this in Step 6.
18. [Connect your GitHub repository to the OSF project](https://help.osf.io/hc/en-us/articles/360019929813-Connect-GitHub-to-a-Project)
19. Add an Open Science statement to the manuscript, with a link to the OSF project
20. Optional: [Publish preprint in a not-for-profit preprint repository such as PsyArchiv, and connect it to your existing OSF project](https://help.osf.io/hc/en-us/articles/360019930533-Upload-a-Preprint)
    + Check [Sherpa Romeo](http://sherpa.ac.uk/romeo/index.php) to be sure that your intended outlet allows the publication of preprints; many journals do, nowadays - and if they do not, it is worth considering other outlets.

<!-- 
4. Optional: Add Materials to repository. It is possible to solicit feedback (through issue/pull request) and later acknowledge outside contributions
-->

# Discussion

In this tutorial paper, we have presented a workflow for open reproducible code in science. The workflow aims to lower the threshold for grass-roots adoption of open science principles. It is accompanied by an R-package with convenience functions, and a WORCS project template for Rstudio. This relatively light-weight workflow meets most of the requirements for Open Science, as detailed in the TOP-guidelines. However, several limitations and issues for future development remain.

One important remaining issue is how to deal with collaborators when using WORCS. Several different scenarios exist. In one scenario, only the lead author uses WORCS. In this case, it is possible to Knit the manuscript to Microsoft Word (.docx), by changing the line `output: papaja::apa6_pdf` in the manuscript's YAML header to `output: papaja::apa6_docx`. There are some limitations to the conversion, discussed [here](https://crsh.github.io/papaja_man/limitations.html#microsoft-word-documents). When soliting feedback from co-authors, ask them to use Tracked Changes and comment bubbles in Word. Then, manually incorporate their changes into the `manuscript.Rmd` file. In most cases, this is the most user-friendly approach, as most lead authors would review changes by co-authors anyway.

A second approach is to send the `manuscript.Rmd` file, and ask them to open (and save) it in Word or Notepad as a plain text file. When they send it back, make sure any changes to your local file are committed, and then simply overwrite your local version with their file. In Rstudio, select the file in the Git tab, and click the Diff button to examine what changes the collaborator has made relative to your last committed version.

A third scenario is when all collaborators are committed to using WORCS. In this case, they can [Fork the repository](https://help.github.com/en/github/getting-started-with-github/fork-a-repo) from the lead author on GitHub, [clone it to their local device](https://help.github.com/en/github/creating-cloning-and-archiving-repositories/cloning-a-repository), make their own changes, and [send you a pull request](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/creating-a-pull-request) to incorporate their changes. Working this way is extremely conducive to scientific collaboration [@ramGitCanFacilitate2013]. Moreover, being familiar with this workflow opens doors to new forms of collaboration. For example, I became a co-author on several R-packages after submitting pull requests with bug fixes or additional functionality. 

Finally, @peikertReproducibleDataAnalysis2019 mention that the [`redoc` package](https://noamross.github.io/redoc/articles/mixed-workflows-with-redoc.html) [@rossRedocReversibleReproducible2020] might be useful for collaborating with Word users. This package enables bidirectional conversion between Word and Rmarkdown, with support for tracked changes. However, a simpler solution (i.e., asking collaborators to work in plain text) might be sufficient.

    - Add to Repo
    - Add Issues for tasks that still need to be accomplished, and tag Collaborators to address them
    - Handle pull requests from collaborators
* advantages
* limitations
* future directions



\newpage

# References
```{r create_r-references}
#r_refs(file = "r-references.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
\setlength{\parskip}{0em}

<div id = "refs"></div>
\endgroup
