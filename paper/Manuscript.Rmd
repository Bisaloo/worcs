---
title: |
    | WORCS: A Workflow for Open Reproducible Code in the Social Sciences and Beyond <!-- AL: I strongly recommend to make the title more specific. "Science" is very general, and in the Anglo-American understanding actually refers to the beta sciences. Also, the abstract and the paper start with talking about the situation in the social sciences and derive the motivaion from there, so it would be good if the title would reflect this. The acronym can stay as it is. ;) -->
    | Version 0.1.0 (Feb 8, 2020)
  
shorttitle        : "WORKFLOW FOR OPEN SCIENCE"

author:
  - name: "Caspar J. Van Lissa"
    institution: "1,2"
    orcid: "0000-0002-0808-5024"
    corresponding: yes
    address: "Padualaan 14, 3584CH Utrecht, The Netherlands"
    email: "c.j.vanlissa@uu.nl"
  - name: "Andreas M. Brandmaier"
    institution: "3,4"
    orcid: "0000-0001-8765-6982"
  - name: "Loek Brinkman"
    institution: "2,5"
    orcid: "0000-0003-3997-1173"
  - name: "Aaron Peikert"
    institution: "3,6"
    orcid: "0000-0001-7813-818X"
  - name: "Barbara M.I. Vreede"
    institution: "7"
  - name: "Anna-Lena Lamprecht"
    institution: "2,8"
    orcid: "0000-0002-5023-4601"

affiliation:
  - id: "1"
    institution: "Utrecht University faculty of Social and Behavioral Sciences, department of Methodology & Statistics"
  - id: "2"
    institution: "Open Science Community Utrecht"
  - id: "3"
    institution: "Center for Lifespan Psychology, Max Planck Institute for Human Development, Berlin, Germany"
  - id: "4"
    institution: "Max Planck UCL Centre for Computational Psychiatry and Ageing Research, Berlin, Germany and London, UK"
  - id: "5"
    institution: "UMC Utrecht"
  - id: "6"
    institution: "Humboldt-Universitaet zu Berlin"
  - id: "7"
    institution: "University Library Utrecht"
  - id: "8"
    institution: "Utrecht University, Department of Information and Computing Sciences"
    
abstract: |
  The social sciences are amidst a paradigm shift towards open science. In part,
  this transition has been fueled by cases of scientific fraud and increasing
  awareness of questionable research practices and failed replications. However,
  open science is not
  merely a cure (or punishment) for this crisis - it is also an opportunity.
  Technological advances enable researchers to more easily conduct reliable,
  cumulative, collaborative science. Capitalizing on these advances has the
  potential to accelerate scientific progress. This tutorial paper introduces a
  workflow for open reproducible code in science (WORCS). WORCS is a 
  step-by-step procedure that researchers can follow to make an entire project
  Open and Reproducible. It is based on best practices, and can be used either
  in parallel to, or in absence of, top-down requirements by journals. WORCS
  lowers the threshold for adopting an open science workflow by providing this
  tutorial, along with an R-package with user-friendly support functions, and an
  RStudio project template.

authornote: |
  This is a preprint, not a peer-reviewed publication. Comments, contributions,
  and coauthors are welcome. Please contact the lead author.
  Acknowledgements: Jeroen Ooms (feedback, suggesting gert, and using Remotes
  field).
keywords          : "open science; reproducibility; r; dynamic document generation; version control"
wordcount         : "5921"

bibliography      : ["worcs.bib"]  

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : yes
mask              : no

class             : "man"
classoption       : "noextraspace"
output            : papaja::apa6_pdf
urlcolor          : blue
knit              : worcs::cite_all
---

```{r load_packages, include = FALSE}
library("papaja")
mask <- TRUE
masked <- function(text){
  ifelse(mask, "[masked]", text)
}
```

```{r analysis_preferences}
# Seed for random number generation
set.seed(42)
```

\setlength{\parskip}{0em}

\vspace{\baselineskip}

<!-- Structure
# Introduction
# Defining open science
* TOP guidelines
# Goal of this paper
* Grass-roots
* open science as opportunity ?move
    - Easier work
    - Less mistakes
    - Better collaboration
    - Cumulative science
    - Error correction
# Introducing the tools
# Setup - do this only once
# WORCS - steps to follow for each project
# Discussion
* advantages
* limitations
* future directions
-->

# Introduction

The (social) sciences are amidst a paradigm shift towards open science. 
Several immediate causes stirred up support for this transition; including several highly publicized cases of scientific fraud [as discussed by @leveltFailingScienceFraudulent2012], increasing awareness of questionable research practices [@johnMeasuringPrevalenceQuestionable2012], and the replication crisis [@shroutPsychologyScienceKnowledge2018].
However, open science is not merely a cure for this crisis - it is also an opportunity [@adolphOpenBehavioralScience2012].
Technological advances enable researchers to more easily conduct reliable, cumulative, collaborative science [@nosekScientificUtopiaOpening2012].
Capitalizing on these advances has the potential to accellerate scientific progress [see also @coyneReplicationInitiativesWill2016].

This paper introduces WORCS: A Workflow for Open Reproducible Code in Science. WORCS is a lightweight approach to open science and computational reproducibility. It provides a step-by-step procedure that researchers can follow to make an entire research project Open and Reproducible. WORCS is based on best practices, and can be used either in parallel to, or in absence of, top-down requirements by journals. The main goal of WORCS is to lower the threshold for adopting an open science workflow by providing the present tutorial, along with an R-package with user-friendly support functions, and an RStudio project template. 

<!-- AL: I second LB's comment somewhere below that the overall WORCS workflow should be introduced much earlier, maybe already here. A diagrammatic overview/flowchart of the principal steps would be great. It should catch the reader's eye and give an immediate idea of what WORCS is about. Now it is indeed quite buried in the details that follow. --> 



<!--
thereby lowering the threshold for adoption.
, and still ensures reproducibility in most circumstances. 
 It is easy to implement, thereby lowering the threshold 

* Grass-roots
* open science as opportunity
    - Easier work
    - Less mistakes
    - Better collaboration
    - Cumulative science
    - Error correction

Benefits to individual scientists: Automation (after a learning curve), thus more productive, avoid mistakes, join projects, clone projects and make them your own.
Collective benefits: Cumulative science, -->
<!-- AL: These "value propositions" would also be good to discuss already in the introduction. Directly after the introduction of the WORCS overview would make sense, I think. -->

The remainder of the paper is structured as follows. ... <!-- AL: Or some variation of this obligatory end-of-intro paragraph. As the paper touches on a lot of aspects, from principal to very practical, it would be very useful to provide some guidance here. -->


# Defining open science

Although Open Science is advocated by many, there is no clear definition of Open Science (LOEK? ref multiple definitions).
The TOP-guidelines have been influential as a concrete operationalisation of Open Science principles [@nosekPromotingOpenResearch2015a].
These guidelines describe eight standards for open science: 1) Comprehensive citation of literature, data, materials, and methods; 2) sharing data, 3) sharing the code required to reproduce analyses, 4) sharing new research materials, and 5) sharing details of the design and analysis; 6) pre-registration of studies before data collection, and 7) pre-registration of the analysis plan prior to analysis; and 8) replication of published results.
WORCS defines the goals of open science in terms of these guidelines, and is designed to facilitate meeting each of these guidelines, with one exception: We do not address replication of published results, because replication relates to the subject of a research project, not to its execution.
We group the remaining seven guidelines into three categories: Citation (1), sharing (2-5), and preregistration (6-7).

## Goal of this paper

The TOP-guidelines are primarily geared towards top-down systemic change (what's in a name) by journals, funders, and organizations.
However, large organizations are slow to change.
For example, a snapshot of editorial practices indicated that, in 2015, only 3% out of 1151 psychology journals explicitly stated that
they accepted replication studies [@martinArePsychologyJournals2017; see also @klebelPeerReviewPreprint2020].
At the same time, there is substantial momentum for grassroots change [@nosekScientificUtopiaOpening2012].
Many individual researchers are willing to adopt best practices for open science, even in the absence of institutional support.
The formation of "Open Science Communities" at universities around the globe illustrates this grassroots movement [REF Loek paper].
This paper is designed as an instrument for grassroots change, that can help individual scientists to meet the principles of open science.
The WORCS workflow can be used regardless of any top-down open science requirements already implemented by journals and institutions:
If such requirements exist, this workflow will help fulfill them.

## Existing solutions

There have been several previous efforts to promote grass-roots adoption of open science principles. Each of these efforts has a different scope, strengths, and limitations that set it apart from WORCS. For example, there are "signalling solutions"; guidelines to structure and incentivize disclosure about open science practices. Specifically, @aalbersbergMakingScienceTransparent2018 suggested publishing a "TOP-statement" as supplemental material, which discloses the authors' adherence to open science principles. Relatedly, @aczelConsensusbasedTransparencyChecklist2019 developed a consensus-based Transparency Checklist that authors can complete online to generate a report. Such signalling solutions are very easy to adopt, and they address TOP-guidelines 1-7.
Many journals now also offer authors the opportunity to earn "badges" for adhering to open science guidelines [@kidwellBadgesAcknowledgeOpen2016]. These signalling solutions help structure authors' disclosures about, and incentivize adherence to, open science practices.

A different class of solutions instead focuses on the practical issue of *how* researchers can meet the requirements of open science. <!--limitation of signalling approaches, that WORCS addresses, is that they do not clarify One could use WORCS to meet the TOP-guidelines, and document this in a TOP-statement.--> One notable example is the workflow for reproducible analyses developed by @peikertReproducibleDataAnalysis2019. Because that workflow focuses on computational reproducibility, it only addresses TOP-guidelines 2, 3, and 5. Moreover, in striving to ensure strict computational reproducibility for even the most sophisticated analyses, Peikert and Brandmaier's workflow ends up being relatively complex to implement. WORCS is designed to be a simple and sufficient solution for projects that can be conducted entirely within R. It eschews much overhead of Peikert and Brandmaier's workflow, resulting in a much more lightweight approach to safeguard computational reproducibility under *most circumstances*. Importantly, WORCS builds upon the same general principles as Peikert and Brandmaier, which means the two workflows are compatible. Finally, WORCS addresses a unique issue not covered by other existing solutions, namely to provide a workflow most conducive to satisfying the TOP-guidelines, while being compatible with existing approaches.

```{r, echo = FALSE, eval = FALSE}
tab1 <- data.frame(
  Reference = c("Aalbers et al., 2018", "Peikert & Brandmaier, 2019"),
  Scope = c("Disclose adherence to open science principles", "Completely reproducible analysis"),
  Difficulty = c("Easy", "Hard"),
  Effectiveness = c("Low", "High"),
  "TOP-guidelines" = c("1-7", "2,3,5")
)
apa_table(tab1)
```

<!-- AL: Similar to my comment on the title, in this section it needs to make clear which scope of related work it covers. For example, in the life sciences there has also been a lot of work on practices for increasing reproducibility, with automated workflows, containerization, Research Objects, provenance protocols, and a lot more. I don't think it's necessary for this paper to discuss this, but that is under the provision that it defines its scope better. --> 
## Prerequisites

Although the principles underlying this workflow are universal, WORCS has been optimized for R [@rcoreteamLanguageEnvironmentStatistical2020].
Several arguments support the choice to work in R.
First, R and all of it extensions are free and Open Source, and Open Source software should be the tool of choice for open science.
Second, as of this writing, R is the most advanced programming language in terms of the implementation of tools required for an open science workflow.
Third, R is the second-most cited statistical software package [@muenchenPopularityDataScience2012], following SPSS, which has no support for any of the open science tools discussed in this paper.
Fourth, R is well-supported; a vibrant online community exists for R, developing new R methods and packages, and providing support and tutorials for existing ones.
Finally, R is highly interoperable:
Packages are available to load nearly every imaginable filetype, and output can be written to most file types, including DOCX and PDF (in APA style), and HTML format. 
Moreover, wrappers are available for many tools developed in other programming languages, and code written in other programming languages can be evaluated from R [including C++, Fortran, and Python, @allaireMarkdownPythonEngine]. There are excellent free resources for learning R, such as the book R for Data Science [@grolemundDataScience2017].

Working with R is simplified immensely by using the RStudio integrated development engine (IDE) for R [@rstudioteamRStudioIntegratedDevelopment2015].
Most of the tools used in WORCS are embedded directly into the user interface of RStudio.
RStudio automates and streamlines tedious or complicated aspects of working with R, Rmarkdown, and Git/GitHub.
This makes RStudio a comprehensive solution for open science research projects.
Another important feature of RStudio is project management.
A project bundles writing, analyses, data, references, etcetera, into a self-contained folder, that can be uploaded entirely to GitHub and downloaded by future users.
The `worcs` R-package installs a new RStudio project template.
When a new project is initialized from this template, the bookkeeping required to set up an open science project is performed automatically.

## Introducing the tools

WORCS relies on several free, open source software solutions.
The first of these is *dynamic document generation*.
Dynamic document generation (DDG) means writing scientific reports in a format
that interleaves written reports with blocks of computer code used to conduct
the analyses. The text is automatically formatted as an APA-style manuscript
[thanks to the R-package `papaja`, @austPapajaPrepareReproducible2020], and
results of the code blocks are automatically generated and insterted in the
text, or rendered as Figures and Tables. Dynamic document generation supercedes
the classical approach of using separate programs to write prose and conduct
analyses, and then manually copy-pasting analysis results into the text.
Although there is a slight learning curve to transitioning to DDG, the
investment really pays off: 

* Time saved from painstakingly copy-pasting output and manually formatting text
soon outweighs the investment of switching to a new program
* Human error in manually copying results is eliminated
* When revisions require major changes to the analyses, all results, Figures and
Tables are automatically updated
* Flexibility in output formats means that a manuscript can be rendered to
presentation format, or even to a website
* Reproducibility is guaranteed, and can be verified by reviewers and/or
readers, because the code is run each time the document is compiled.

In sum, while writing academic papers in a programming environment might seem
counter-intuitive at first, this approach is much more amenable to the needs of
academics than most word processing software. It prevents mistakes, and saves
time. We recommend centering a research project around one dynamically generated
(RMarkdown) document, which includes all analysis code. Longer scripts can be
stored in `.R` files, and called from the main document using the `source()`
function.

The second solution is *version control*:
Maintaining an indelible log of every change to all project files. Version 
control is a near-essential tool for scientific reproducibility, as anyone
learns who has had the experience of accidentally deleting a crucial file, or of
being unable to reproduce analyses because they ran some analyses interactively
instead of using syntax [see also @blischakQuickIntroductionVersion2016].
Many scientists use some form of *implicit* version control; for example, by
renaming files after major changes (e.g., "manuscript_final_2.2-2019-10-12.doc"),
tracking changes in word processing software, or using cloud hosting services
that retain backups of previous versions.
WORCS instead uses the *explicit* version control software 'Git'
(www.git-scm.com). Git tracks
changes to files, and stores these changes when the user makes a "commit" (a 
snapshot of the version controlled files).
<!--Barbara: I have looked for a way to cite Git but did not find one... 
Seems good practice especially in this paper as an example to cite all software,
but for Git I just cannot find the way. Maybe cite the website: www.git-scm.com
CJ: Other papers discussing scientific merits of Git also just give a URL -->
Git retains a complete log of all commits, and users can compare changes between
different commits, or go back to a previous version of the code (for example,
after making a mistake, or to replicate a previous version of the results).
A project version controlled with Git is called a "Repository", or Repo.
Git only version controls files explicitly committed by the user. Moreover, it
is possible to prevent files from being version controlled - which is useful for
privacy sensitive data.
The `.gitignore` file lists files that should not be version-controlled.
These files thus exist only on the user's private computer.

The functionality of 'Git' is amplified by services such as 'GitHub'
(https://github.com). 'GitHub' is best understood as a cloud storage service
with social networking functionality. The cloud storage aspect
of 'GitHub' works as follows: You can "clone" (copy) a local 'Git' repository to
the 'GitHub' website, as a backup or research archive. The social network aspect
comes into play when a repository on 'GitHub' is set to "public": This allows 
other researchers to peruse the repository and see how the work was done, clone
it to their own computer to replicate the original work or apply the methods to
their own data, open "Issues" to ask questions or give feedback on the project,
or even send a "Pull request" with suggested changes to the text or code for
your consideration. 'Git' and 'GitHub' shine as tools for collaboration, because
different people can simultaneously work on different parts of a project, and
their changes can be compared and automatically merged on the website. Even on
solo projects, working with 'Git'/'GitHub' has many benefits: Staying organized,
being able to start a new study with a clone of an old, similar repository, or
splitting off an "experimental branch" to try something new, while retaining the
ability to "revert" (return) to a previous state of the project, or to "merge"
(incorporate) the experimental branch. Although this paper focuses on 'GitHub',
the two main alternative 'Git' cloud storage services, 'GitLab'
(https://gitlab.com) and 'Bitbucket' (https://bitbucket.org), can interchangeably
be used in `worcs` projects (as of version 0.1.2). The package has a 
[vignette]() with additional instructions for using these platforms.

The third solution is *dependency management*: Keeping track of exactly what
software was used to conduct the analyses. At first glance, it might seem
sufficient to state that analyses were conducted in *Program X*. However, every
program is susceptible to changes, updates, and bugfixes. R-packages in
particular are updated frequently, because they are Open Source, and there is an
active community of developers contributing functionality and bugfixes. 
Potentially, any update could change the results of the code. For example, 
R-version 3.6.0 fixed a bug in the random number generator. This means that any
analyses that use random numbers, such as bootstrapping, cannot be strictly
replicated from previous versions. Or, for example, R-version 4.0.0 changed a
default parameter that affects how data are loaded from files.

<!-- LB: and onwards: this section requires some background knowledge on R/packages/etc. This could be a place where readers stumble and stop reading. Perhaps introduce the concept of the R project template earlier, to see section on dependencies more into perspective. 
[main issue] it should become clear earlier what the WORCS workflow entails: e.g. switching to working with a particular R Project Template. Discuss the ‘workflow' section earlier. Related: Can you make a schematic overview of the WORCS?overview--> 
Many solutions exist to ensure computational reproducibility. These solutions typically work by
enveloping your research project in a distinct "environment" that only has access to programs that
are explicitly installed, and maintaining a record of these programs. These solutions differ in
user-friendliness and effectiveness. If strict reproducibility is required, one might use the
aforementioned Docker-based workflow by @peikertReproducibleDataAnalysis2019, which runs all
analyses inside a "container": An environment that behaves like a virtual computer that can be
stored like a sort of time capsule, and identically reinstated on a user computer, or in the cloud.
This is effective, and also preserves software outside the R environment (e.g., preprocessing tools
for neuroimaging data). However, it can be difficult to set up for novice users - although a package
is in development to facilitate this transition [@repo]. The cloud-based research collaboration platform
'Code Ocean' is also based on Docker, for example. It is user-friendly, but it is not clear whether the
["Data Processing Agreement"](https://www.digitalocean.com/legal/data-processing-agreement/) is fully
GDPR compliant - it seems to focus more on limiting liability than on compliance. This is a risk with
any cloud-based solution. The advantage of using a local system is that data sharing is optional, and
WORCS capitalizes on this flexibility. 

A slightly less sophisticated solution is offered by the R-package `packrat`, which installs all software
used in a project to a dedicated library that can be stored and copied with the project. This solution
is reasonably user-friendly and effective, but has the disadvantage that for each project, all packages
must be installed anew, which can take a long time (sometimes hours) and a lot of hard drive space.
Moreover, Git/GitHub is not designed to store your entire project library, so `packrat` does not address
the issue of how other users are supposed to get access to your library. In conclusion, `packrat` is
somewhat unwieldy. The package `checkpoint` offers an extremely lightweight solution: Instead of storing
all software used, it simply records the calendar date, and then installs the version of all required
packages that was available on the central R repository `CRAN` on that date from an archival website.
Thus, all that is needed to restore a project is the date on which it was conducted. This is very
user-friendly, and reasonably effective - the only problem is that this solution is limited to `CRAN`
packages, and cannot mix package versions from different dates.

A recently developed package strikes a great balance between user-friendliness and effectiveness: `renv`, developed by the team behind RStudio. This package maintains a text-based log file of which packages are used, in what versions, and where they were installed from (e.g., CRAN, Bioconductor, GitHub). Being text-based, this log can be version controlled with Git. On a local computer, `renv` installs all packages used in a cache that is shared between all projects that use `renv`. Thus, if one of your projects uses a version of a package that is already in the cache, it does not need to be installed again. This overcomes the limitations of long installation times and large space requirements of `renv`. When someone else loads your project, `renv` will install all of the required packages onto a cache on their computer. Whereas `packrat` copies the source code of all packages, `renv` obtains the packages from their original repositories. Thus, as long as these repositories used maintain their archives, this solution is very lightweight and effective. It is not foolproof, but some have argued that all solutions for computational reproducibility have a limited shelf life [@brownHowLearnedStop2017]. Ultimately, the best solution might be to use a good-enough solution, like `renv`, and acknowledge that all code requires some maintenance if you want to reproduce it in the future.

## Considering filetypes
A key consideration when developing a research project is what filetypes to use.
A case can be made to use text-based files only (or as much as possible), instead of binary files.
Text-based files can be read by machines and humans alike, without requiring special (licensed) software.
Binary files, such as Word (`.docx`) or SPSS (`.sav`, `.spo`),  must be decoded by software first.
Git is designed to keep track of changes to the content of text-based files.
Each change reflects a word or line of code added or removed.
Binary files, on the other hand, should not be version controlled with Git, as individual changes in the encrypted file are uninterpretable.
Moreover, text-based files are often smaller than binary files, which means they take up less space on cloud hosting services, such as GitHub. Uploading large binary files is frowned upon.
GitHub also allows visitors to peruse text-based files online, and conveniently renders certain filetypes.
For example, GitHub highlights syntax in `.R` and `.Rmd` files, and displays `.csv` files as spreadsheets.

<!--Barbara: Maybe write here something about writing manuscripts on a line-by-line basis, so that version control can keep track of changes more accurately than if a whole paragraph is identified as altered?-->

## Open science as opportunity

<!-- LB:  this section comes at a weird place in the flow of the paper. 
Barbara: agreed! I would remove this altogether.-->
Sometimes, open science principles are discussed as a kind of "antidote" to scientific misconduct. Yet these principles also present an opportunity to conduct more reliable research, with fewer mistakes, more convenient collaboration, more effective internal and external collaboration, and the potential for crowd-sourced error correction. 

# How WORCS relates to the TOP-guidelines

## Comprehensive citation

TOP encourages comprehensive citation of literature, data, materials, methods, and software. In principle, researchers can meet this requirement by simply citing every reference used. Unfortunately, citation of data and software is less commonplace than citation of literature and materials. Crediting these resources is important, because it incentivizes data sharing and the development of open-source software, and supports the open science efforts of others. 

<!--Various initiatives are underway to innovate how scientific output is measured and valued. https://www.nwo.nl/beleid/statement+waarderen+en+belonen+van+wetenschappers --> 
To facilitate citing datasets, researchers sometimes publish *data papers*; documents that detail the procedure, sample, and codebook. Specialized journals, such as the [*Journal of Open Psychology Data*](https://openpsychologydata.metajnl.com/), aid in the publication of these data papers. For smaller projects, researchers sometimes publish their data in an online repository - like GitHub - along with a text file with the preferred citation for the data (which can be a substantive paper), and the license that applies to the data, such as Creative Commons [BY-SA](https://creativecommons.org/licenses/by-sa/4.0/) or [BY-NC-SA](https://creativecommons.org/licenses/by-nc-sa/4.0/). When in doubt, one can always contact the data creators and ask what the preferred citation is. 

<!-- AL: Some general things about software citation that could be good to mention here: Tool or software papers are also common, often as short papers, sometimes called "Application Notes". The Journal of Open Source Software (JOSS) is a way of turning software directly into a publication, without the need of writing an actual paper about it (the review is about the code). https://citation-file-format.github.io/ is a file format to include software citation information into source code repositories.--> 
When using R, software (R packages) can be conveniently cited by calling `citation("packagename")`. This returns an APA-style reference, and a BibTeX entry. However, users of the WORCS project template for RStudio do not need to manually cite R-packages. WORCS generates a template manuscript in APA6 style, based on the `papaja` package [@austPapajaPrepareReproducible2020], which automatically cites all packages used.

One important impediment to comprehensive citation is the fact that print journals operate with space constraints.
Print journals often discourage comprehensive citation, either actively, or passively by including the reference list in the manuscript word count.
Researchers can overcome this impediment by preparing two versions of the manuscript:
One version with comprehensive citations for online dissemination, and another version for print, with only the essential citations.
The print version should reference the online version, so interested readers can find the comprehensive reference list.
Step 20 of the WORCS procedure suggests uploading the online version to a preprint server.
This is important because most preprint servers - at least those hosted by the [Open Science Framework (OSF)](https://help.osf.io/hc/en-us/articles/360019930493-Preprint-FAQs#how_do_i_find_preprints) - are indexed by Google Scholar.
This means that authors will receive credit for cited work; even if they are cited only in the online version.
Moreover, preprint servers ensure that the online version will have a persistent DOI, and will remain reliably accessible, just like the print version. 

It is easiest to mark the distinction between essential and non-essential references from the start, instead of going
back to cut references prior to publication. Therefore, the `worcs` package distinguishes between the
traditional at-symbol (`@`) used to cite a reference, and the "double at"-symbol (`@@`), used to cite a non-essential reference.
Users can render a manuscript to PDF either with, or without, comprehensive citations.

The procedure for citation in WORCS is as follows:<!-- LB: this procedure is secondary to the main workflow. Consider changing the order (see main issue)-->

1. During writing, maintain a plain-text `.bib` file with the BibTeX references for all citations.
    + You can make this file by hand; e.g., Figure \@ref(fig:scholarbib) shows how to obtain a BibTeX reference from Google Scholar; simply copy-paste each reference into the `.bib` file
    + You can export a `.bib` file from most reference manager programs; the free, open-source reference manager [Zotero](https://www.zotero.org/download/) is excellent and user-friendly, and highly interoperable with other commercial reference managers. [Here](https://christopherjunk.netlify.com/blog/2019/02/25/zotero-rmarkdown/) is a tutorial for using Zotero with Rmarkdown.
2. To cite a reference, use the `citekey` - the first word in the BibTeX entry for that reference. Insert it in the Rmarkdown file like so: `@yourcitekey2020`. For a parenthesized reference, use `[@citekeyone2020; @citekeytwo2020]`. For more options, see the [Rmarkdown cookbook](https://bookdown.org/yihui/rmarkdown-cookbook/bibliography.html).
3. To indicate a *non-essential* citation, mark it with a double at-symbol: `@nonessential2020`.
4. When Knitting the document, adapt the `knit` command in the YAML header.  
  `knit: worcs::cite_all` renders all citations, and  
  `knit: worcs::cite_essential` removes all *non-essential* citations.
5. Optional: To be extremely thorough, you could make a "branch" of the GitHub repository for the print version of the manuscript. Only in this branch, you use the function `knit: worcs::cite_essential`. The procedure is documented in [this tutorial](http://rstudio-pubs-static.s3.amazonaws.com/142364_3b344a38149b465c8ebc9a8cd2eee3aa.html).

```{r scholarbib, echo = FALSE}
knitr::include_graphics("scholar_bib.png")
```

<!--
Future: Explain why each reference is cited? How best to include this?
* Note
* In a function
* In a separate document-->

## Data sharing

Data sharing is important for computational reproducibility and secondary analysis. Computational reproducibility means that a third party can exactly recreate the results from the original data, using the published analysis code. Secondary analysis means that a third party can conduct sensitivity analyses, explore alternative explanations, or even use existing data to answer a different research question. From an open science perspective, data sharing is always desirable. From a practical point of view, it is not always possible. Data sharing may be impeded by legal constraints, ethical restrictions, absence of informed consent for data sharing, and privacy concerns. For example, one concern that applies when working with human participant data, is pseudonimization. It is crucial to pseudonimize data as soon as they are collected, by removing or deleting any sensitive personal information and contact details. Pseudononimized data can often be shared if participants have provided informed consent to that effect. Nevertheles, it is important to note that the European GDPR prohibits storing 'personal data' (information which can identify a natural person whether directly or indirectly) on a server outside the EU, unless it offers an "adequate level of protection". Although different rules may apply to pseudonimized data, there are many repositories that are GDPR compliant, such as the European servers of the Open Science Framework. Different Universities, countries, and funding bodies also have their own repositories that are complient with local legistation. Before sharing any human participant data, it is recommended to obtain approval from an internal (ethical) review board, guidance from Research Data Management Support, and informed consent from participants.

<!-- LB: As I reader, I would prefer to hear about data sharing options before the synthetic data part (which is only relevant to those who cannot share their data). E.g. can I indicate a point in my workflow (e.g. after removing some privacy sensitive columns of my data) to share the as-raw-as-possible data ?-->
If data can be shared openly, it can simply be committed it to the 'Git'
repository, along with the analysis code. This way, others can download the
entire repository and reproduce the analyses from start to finish. 
If data cannot be shared, researchers should aim to safeguard the potential for
computational reproducibility and secondary analysis as much as possible. The
`worcs` package uses two solutions to accomplish this goal.
The first solution is to publish a *checksum* of the original data file.
Think of a checksum as a 32-character summary, or as a "one word" description,
of the contents of a file.
Any change to the file will result in a different checksum. 
Thus, one can use a checksum to verify the identity of a file, and ensure that its contents are unchanged.
When data cannot be shared, the risk of fraud or abuse can be mitigated by publishing a checksum for the original data, in addition to the complete analysis code.
Using the checksum to verify the identity of a private dataset, researchers can prove to an independent auditer that running the public analysis code on the private data results in the published results of their work.

The second solution implemented in `worcs` is to share a synthetic dataset with similar characteristics to the real data. Synthetic data mimic the level of measurement and (conditional) distributions of the real data [see @nowokSynthpopBespokeCreation2016]. Sharing synthetic data allows any third party to 1) verify that the published code works, 2) debug the code, and 3) write valid code for alternative analyses. It is important to note that complex multivariate relationships present in the real data are often lost in synthetic data. Thus, findings from the real data might not be replicated in the synthetic data, and findings in the synthetic data *should not be substantively interpreted*. Still, sharing synthetic data facilitates data requests from third parties. A third party can write analysis code based on the synthetic data, and send it to the authors who evaluate it on the real data and send back the results. 

### Processing data in WORCS

Regardless of whether the data will be open or closed, it is important that the raw data will be left unchanged as much as possible. This eliminates human error. Any alterations to the data - including processing steps and even error corrections - should be documented in the code, instead of applied to the raw data. This way, the code will be a pipeline from the raw data to the published results. Every `worcs` project contains an empty R-script called `prepare_data.R`. As soon as the data are collected, researchers should use this file to document the steps necessary to load the data into R, pseudonimize it, and prepare it for analysis. For example, if the data was originally in SPSS format with IP addresses and GPS location, this file might just contain the code required to read the SPSS file, and to remove those columns of sensitive personal information. As soon as the data are pseudonimized and processed into a 'tidy' format [i.e., data that can be stored as a spreadsheet, see @wickhamTidyData2014], the researcher should version control some indelible record of the raw data, and avoid making any further changes to the version controlled file.

The `worcs` package offers two functions for version controlling a record of the data: One for open, and one for closed data:

* `open_data(data)`: This function stores the `data` object (a `data.frame` or `matrix`) in a spreadsheet file (called `data.csv` by default).
This file should now be (manually) added to the 'Git' repository, committed, and pushed to GitHub.
Once the GitHub repository is made public, these data will be open to the public.
Assume that, once you do this, the data cannot be un-shared.
* `closed_data(data)`: This function also stores the data in a local file (called `data.csv` by default), but that file is added to the `.gitignore` file so it cannot be accidentally added to the 'Git' repository.
The function also adds a checksum for the original data in the `.worcs` project file, and creates a synthetic dataset (called `synthetic_data.csv` by default).
The user should commit all changed files and push them to GitHub.
Once the GitHub repository is made public, people will have access to a checksum for the original data that exist on your local machine, a codebook describing those data, and a synthetic copy of the data.
<!-- Maybe remark that in the "closed" case, a the data should be backupped somewhere else, as GitHub will not do the job in this setting. -->

The final line of the `prepare_data.R` file should thus be either `open_data()` or `closed_data()`. The analysis code, on the other hand, should begin with `load_data()`. This function loads the real data if it is available on the user's computer, and otherwise, loads the synthetic data. This will have the effect that third party users who copy the GitHub repository will automatically load the synthetic dataset, whereas the study authors, who have the real data stored locally, will automatically load the real data. This function makes it possible for reviewers, coauthors, and auditors, to write analysis scripts without requiring access to the original data. They can simply start the script with `load_data()`, and write their code based on the synthetic data. Then, they can submit their code to you - by email or as a ["Pull request"](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/creating-a-pull-request) on GitHub. When you run their code on your system, `load_data()` will detect the original data, and use that to run their script. You can then return the results to the third party.

As of version 0.1.1, the `worcs` package only stores data as `.csv` files. As explained above, this is intentional, because `.csv` files are human- and machine readable, and because data typically need to be in a spreadsheet format for analysis. Many types of data can be represented as a spreadsheet - text corpora can have one row per document, and EEG- or EKG waveforms can have one row per measurement occasion. The `prepare_data.R` file should document any steps required to convert these data into tabular format. If it is data must be stored in a different file type anyway, readers are encouraged to follow the development of the `repro` package [@repro], which will enable such enhancements.

Some users may intend to make their data openly available through a restricted access platform, such as an institutional repository, but not publically through GitHub. In this case, it is recommended to use `closed_data()`, and to manually upload the original `data.csv` file to the restricted access platform. If users wish to share their data through the Open Science Framework, it is sufficient to connect the OSF page to the GitHub repository as an [Add-on](https://help.osf.io/hc/en-us/categories/360001550973-Add-ons).

## Sharing code, research materials, design and analysis

When writing a manuscript in Rmarkdown, analysis code is embedded in the prose of the paper. Thus, the TOP-guideline of sharing analysis code can be met simply by committing the source code of the manuscript to GitHub, and making this GitHub repository Public upon publication. If authors additionally use open data and a reproducible environment (default in WORCS), then a third party can simply replicate all analyses by copying the entire GitHub repository, and Knitting the manuscript on their local computer.

Aside from analysis code, the TOP-guidelines also encourage sharing new research materials, and details of the study design and analysis. These goals can be accomplished simply by placing any relevant documents in the RStudio project folder, committing them to the Git repository, and pushing to GitHub. As with any document version controlled in this way, it is advisable to use plain text only.

## Preregistration

Lindsay and colleagues [-@lindsayResearchPreregistration1012016] define preregistration as "creating a permanent record of your study plans before you look at the data. The plan is stored in a date-stamped, uneditable file in a secure online archive." Two such archives are well-known in the social sciences: AsPredicted.org, and OSF.io. However, GitHub also conforms to these standards. Thus, it is possible to preregister a study simply by pushing a text document with the study plans to GitHub. If the GitHub repository is Public, the preregistration is immediately visible to the world, and Reviewers can submit comments through pull requests. If the repository is Private, Reviewers can be invited as "Collaborators", although this removes their anonymity. Private repositories can be made public at a later date, along with their entire time-stamped history. 
<!-- Barbara: Pull requests are not anonymous either, aren't they?-->

The advantages, disadvantages, and pitfalls for preregistering different types of studies have been extensively debated, and summarizing that debate falls outside of the scope of this paper, and has been addressed adequately elsewhere [e.g., @lindsayResearchPreregistration1012016]. We take the pragmatic position that, in confirmatory (hypothesis-testing) research, it is beneficial to plan research projects before executing them, to preregister these plans, and adhere to them. All deviations from this procedure should be disclosed; e.g., prior exposure to the data, whether direct (e.g., by computing summary statistics) or indirect (e.g., by reading papers using the same data); deviations from the analysis plan to handle unforseen contingencies, such as violations of model assumptions; or additional exploratory analyses.

WORCS uses the following approach to preregistration:

1. Document study plans in a `preregistration.Rmd` file, and planned analyses in a `.R` file.
2. Commit these documents to the local Git repository, and push them to GitHub
3. On GitHub, [tag the release](https://help.github.com/en/github/administering-a-repository/creating-releases) as "Preregistration". A tagged release helps others retrieve this commit.
4. Optional: Upload the files as attachments to another pre-registration repository, e.g., AsPredicted.org or OSF.io.
  + The `.Rmd` file can be Knitted to a PDF form before uploading to a repository


### Preregistering study plans

Several excellent preregistration templates are available in the R-package `prereg` [@austPreregMarkdownTemplates2019], including templates from organizations like [AsPredicted.org](AsPredicted.org) and [OSF.io](OSF.io), and from researchers [e.g., @vantveerPreregistrationSocialPsychology2016]. When opening a new RStudio project with the WORCS project template, it is possible to select one of these preregistration templates, which will generate a file called `preregistration.Rmd`. This file should be used to document study plans, ideally prior to data collection.

Because the practice of preregistration is strongly tied to the collection of primary data, it has been a matter of some debate whether secondary data analyses can be preregistered [but see @westonRecommendationsIncreasingTransparency2019 for an excellent discussion of the topic]. Preregistration is the only way to ensure that a researcher is not "HARKing": Hypothesizing after the results are known [@kerrHARKingHypothesizingResults1998]. When analyzing existing data, it is difficult to *prove* that a researcher did not have direct (or indirect, through collaborators or by reading studies using the same data) exposure to the data, before composing the preregistration. However, the question of proof is only relevant from a perspective of preventing scientific misconduct. Good faith preregistration efforts always improve the quality of deductive (theory-testing) research, because they avoid HARKing, ensure reliable significance tests, avoid overfitting noise in the data, and limit the number of forking paths researchers wander during data analysis [@gelmanStatisticalCrisisScience2014]. In all cases, researchers should minimize exposure to the data, and disclose any exposure.

### Preregistering analyses

The ideal preregistered analysis consists of a complete analysis script that can be evaluated once the data are obtained.
This ideal is often unattainable, because the data present researchers with unanticipated challenges; e.g., assumptions are violated, or analyses work differently than expected.
Some of these challenges can be avoided by simulating the data one expects to obtain, and writing the analysis syntax based on the simulated data.
This topic is beyond the scope of the present paper, but many user-friendly methods for simulating data are available in `R`, including the package [`simstudy`](https://cran.r-project.org/web/packages/simstudy/vignettes/simstudy.html).
<!-- citation:

  @Manual{,
    title = {simstudy: Simulation of Study Data},
    author = {Keith Goldfeld},
    year = {2020},
    note = {R package version 0.1.16},
    url = {https://CRAN.R-project.org/package=simstudy},
  }
  -->
When using the WORCS-workflow, it is recommended to preregister, if possible, a `.R` script with the planned analyses, along with a verbal, conceptual description.
If any changes must be made to the R-script after obtaining the data, one can refer to the conceptual description to explain why these changes are necessary.


<!--
3. Write conceptual preregistration
* https://osf.io/zab38/wiki/home/ Templates of OSF Registration Forms
* R package prereg
* mention other examples and cite van 't Veer, A. E., & Giner-Sorolla, R. (2016). Pre-registration in social psychology---A discussion and suggested template. Journal of Experimental Social Psychology, 67, 2--12. doi: 10.1016/j.jesp.2016.03.004
3. Commit, push, and tag th as preregistration
Preregistration: tag on GitHub
https://stackoverflow.com/questions/18216991/create-a-tag-in-a-github-repository

4. Optional: Add Materials to repository. It is possible to solicit feedback (through issue/pull request) and later acknowledge outside contributions
4. Collect data and either add to repository, or register hash of original datafile
5. Conduct analyses and write; commit regularly (ideally, after completing a small and clearly defined task), with informative commit messages. Call renv::snapshot() to save the state of the project library to the lockfile (called renv.lock),
6. Create OSF project, connect to GitHub, mention OSF page in manuscript
7. Optional: Publish preprint in a not-for-profit preprint repository (check Sherpa Romeo) http://sherpa.ac.uk/romeo/index.php
7. Submit
8. Make repository public -->


# The workflow 
<!-- AL: Agreeing with LB's ealier comment, it would be good to discuss this overall workflow before going into the details. --> 

## Preparing your system

Before you can use the WORCS workflow, you have to install the required software, and register for a GitHub account.The setup process is documented in a vignette included with the `worcs` R-package, and also [available online](https://cjvanlissa.github.io/worcs/articles/setup.html). After preparing your system, you can follow the step-by-step procedure outlined below.
<!--Barbara: Does this work on gitlab/bitbucket as well?-->

## Phase 1: Study design

1. Create a new Private repository on GitHub.
<!-- Why private!?! I would actually argue for a public repo, but if you do not feel comfortable doing so: another option would be to give an overview of advantages/disadvantages of private/public here, and make an argument for either case.-->
Do not change any of the default options. Copy the repository link to the clipboard. This link will look like https://github.com/yourname/yourrepo.git
2. In RStudio, click File > New Project > New directory > WORCS Project Template
    a. Paste the GitHub Repository address in the textbox
    b. Keep the checkbox for `renv` checked if you want to document all R-package dependencies, and have them automatically loaded on any system (recommended).
    c. Select a preregistration template
    d. Click Open in new session, and Create project
3. Write the preregistration `preregistration.Rmd`
4. In the top-right corner of RStudio, select the Git tab, select the checkboxes next to all files, and click the Commit button. Write an informative message for the commit, e.g., "Preregistration", again click Commit, and then click the green Push arrow to send your commit to GitHub
5. Go to the GitHub repository for this project, and tag the Commit as a preregistration
6. Optional: Render the preregistration to PDF, and upload it to AsPredicted.org or OSF.io as an attachment
7. Optional: Add study Materials (only those to which you own the rights!) to the repository.
It is possible to solicit feedback (by opening a GitHub Issue) and receive outside contributions (by accepting Pull requests)
<!-- Barbara: NB this can only be done in a public repo; another argument for making a repo public! -->

## Phase 2: Data analysis

8. Read the data into R, and document this procedure in `prepare_data.R`
9. Use `open_data()` or `closed_data()` to store the data
10. Write the manuscript in `Manuscript.Rmd`, using code chunks to perform the analyses.
11. Regularly commit your progress to the Git repository; ideally, after completing each small and clearly defined task. Use informative commit messages. Push the commits to GitHub.
12. Cite essential references with one at-symbol (`[@essentialref2020]`), and non-essential references with a double at-symbol (`[@nonessential2020]`).
<!-- Barbara: Again I cannot see the difference... -->

## Phase 3: Paper submission

13. Save the state of the project library (all packages used), call `renv::snapshot()`. This updates the lockfile, `renv.lock`.
14. Add an open science statement to the manuscript, which references the online repository for the project. This reference can be masked for blind review. The open science statement should indicate whether additional references are available in the online repository; whether data, code, materials, and study design details are shared, and whether a pre-registration is available. For further guidance, see @aalbersbergMakingScienceTransparent2018. Example:  
    > In the spirit of open science, an online repository is available at XXX, which contains more comprehensive citations, [the data/a synthetic data file], analysis code, the research materials used, details about the study design, and a tagged commit with a preregistration.

15. To render the paper with essential citations only for submission, change the line `knit: worcs::cite_all` to `knit: worcs::cite_essential`. Then, press the Knit button to generate a PDF

## Phase 4: Publication

Note that some researchers take the following steps at an earlier stage. However, for newcomers to open science, who are unsure about potential implications for the publication of their work, it might be prudent to perform these actions after acceptance, and before publication:

16. Make the GitHub repository public
17. [Create an OSF project](https://help.osf.io/hc/en-us/articles/360019737594-Create-a-Project); although you may have already done this in Step 6.
18. [Connect your GitHub repository to the OSF project](https://help.osf.io/hc/en-us/articles/360019929813-Connect-GitHub-to-a-Project)
19. Add an open science statement to the manuscript, with a link to the OSF project
20. Optional: [Publish preprint in a not-for-profit preprint repository such as PsyArchiv, and connect it to your existing OSF project](https://help.osf.io/hc/en-us/articles/360019930533-Upload-a-Preprint)
    + Check [Sherpa Romeo](http://sherpa.ac.uk/romeo/index.php) to be sure that your intended outlet allows the publication of preprints; many journals do, nowadays - and if they do not, it is worth considering other outlets.

<!-- 
4. Optional: Add Materials to repository. It is possible to solicit feedback (through issue/pull request) and later acknowledge outside contributions
-->

# Discussion

In this tutorial paper, we have presented a workflow for open reproducible code in science. The workflow aims to lower the threshold for grass-roots adoption of open science principles. It is accompanied by an R-package with convenience functions, and a WORCS project template for RStudio. This relatively light-weight workflow meets most of the requirements for open science, as detailed in the TOP-guidelines. However, several limitations and issues for future development remain.

One potential challenge is the learning curve associated with the tools outlined in this paper. Learning to work with R, Rmarkdown, and Git/GitHub requires an initial time investment. The amount of time required is reduced by the availability of tutorials, such as this one, and other tutorials cited throughout this document. Moreover, the time investment tends to pay off. Working with R opens the door to many cutting edge analysis techniques. Working with Rmarkdown saves time and prevents mistakes by avoiding tedious copying of results into a text document. Working with Git/GitHub keeps projects organized, prevents accidental loss of work, enables integrating changes by collaborators in a non-destructive manner, and ensures that entire research projects are archived and can be accessed or copied by third parties. Thus, the time investment is eminently worthwhile.

Another important challenge is managing collaborations when only the lead author uses Rmarkdown, and the coauthors use Word. In this case, it is possible to Knit the manuscript to Word (.docx), by changing the line `output: papaja::apa6_pdf` in the manuscript's YAML header to `output: papaja::apa6_docx`. There are some limitations to the conversion, discussed [here](https://crsh.github.io/papaja_man/limitations.html#microsoft-word-documents). When soliting feedback from co-authors, ask them to use Tracked Changes and comment bubbles in Word. <!-- LB: The work-around with track-changes in Word files is doable, but not very satisfying. I think you can also use Overleaf instead of Word (user friendly and web based) for bidirectional communication, but haven’t tried this myself. I didn’t know about ‘redoc’, will check it out.-->Then, manually<!--this should really be avoided if possible! no fun... --> incorporate their changes into the `manuscript.Rmd` file. In most cases, this is the most user-friendly approach, as most lead authors would review changes by co-authors anyway. A second approach is to ask collaborators to work in plain text. In this case, send collaborators the `manuscript.Rmd` file, and ask them to open (and save) it in Word or Notepad as a plain text file. When they send it back, make sure any changes to your local file are committed, and then simply overwrite your local version with their file. In RStudio, select the file in the Git tab, and click the Diff button to examine what changes the collaborator has made relative to your last committed version. Finally, @peikertReproducibleDataAnalysis2019 mention that the [`redoc` package](https://noamross.github.io/redoc/articles/mixed-workflows-with-redoc.html) [@rossRedocReversibleReproducible2020] might be useful for collaborating with Word users. This package enables bidirectional conversion between Word and Rmarkdown, with support for tracked changes. However, a simpler solution (i.e., asking collaborators to work in plain text) might be sufficient.

If all collaborators are committed to using WORCS, they can [Fork the repository](https://help.github.com/en/github/getting-started-with-github/fork-a-repo) from the lead author on GitHub, [clone it to their local device](https://help.github.com/en/github/creating-cloning-and-archiving-repositories/cloning-a-repository), make their own changes, and [send a pull request](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/creating-a-pull-request) to incorporate their changes. Working this way is extremely conducive to scientific collaboration [@ramGitCanFacilitate2013]. Moreover, being familiar with GitHub opens doors to new forms of collaboration: In the open source software community, continuous peer review and voluntary collaborative acts by strangers who are interested in a project are commonplace [see @adolphOpenBehavioralScience2012].  This kind of collaboration is budding in scientific software development as well; for example, I <!-- AL: which of the meanwhile several authors? :) -->  became a co-author on several R-packages after submitting pull requests with bug fixes or additional functionality [@rosenbergTidyLPAPackageEasily2018; @hallquistMplusAutomationPackageFacilitating2018]. It is also possible to invite such collaboration by [opening Issues](https://help.github.com/en/github/managing-your-work-on-github/about-issues) for tasks that still need to be accomplished, and tag known Collaborators to address them, or invite external collaborators to contribute their expertise. 

## Future developments

The WORCS procedure and accompanying R-package provide a user-friendly and lightweight workflow for open, reproducible research in R, that meets all TOP-guidelines. It is beyond the scope of WORCS to support the incorporation of tools outside of the R-environment, or to containerize a project so that it can be identically reinstated on a different system or virtual machine. One important area of future development is to enable these extensions of the workflow. To this end, we are integrating WORCS into a modular framework of reproducibility tools for R [@repro]. This will make it possible for researchers to enhance a WORCS project with strict reproducibility tools like 'Docker' and 'Make', or vice versa, to initiate a WORCS project in a strictly reproducible environment. This combines the strengths of WORCS with those of @peikertReproducibleDataAnalysis2019.

<!-- Open peer review? -->

## Conclusion

WORCS offers a workflow for open reproducible code in science. The step-by-step procedure outlined in this tutorial helps researchers make an entire research project Open and Reproducible. The accompanying R-package provides user-friendly support functions for several steps in the workflow, and an RStudio project template to get the project set up correctly.

WORCS encourages and simplifies the adoption of Open Science principles in daily scientific work. It helps to make all research objects that are created through the scientific process (data and code, but also manuscripts) open and publicly assessible. This enables other researchers to reproduce results, but also provides a proper basis for research evaluation according to the San Francisco Declaration on Research Assessment (DORA) [https://sfdora.org/], which is increasingly adopted by research institutions around the globe. For example, reviewers of grant proposals have immediate access to previous work of a researcher or a group of researchers, and can assess research quality based on content rather than relying on spurious surrogate indicators like personal h-indexes, journal impact factors and conference rankings. The detailed version control and commit tracking of github and similar platforms furthermore make it possible to assess the contributions made by different researchers, which can for example play a role in hiring or promotion procedures. 

The FAIR Guiding Principles for scientific data management and stewardship [https://doi.org/10.1038/sdata.2016.18] advocate that all digital research objects should be Findable, Accessible, Interoperable and Reusable. They are increasingly adopted by research organizations and funders as a requirement towards more open, transparent and reproducible science. The WORCS workflow includes a step to publish the original raw data (if possible) or alternatively a synthetic data set in the (public) project repository, thus making the research data or a workable substitute accessible. WORCS uses CSV as an open and widely used, tabular format for data representation, thus enabling interoperability with other data sets and software. The R scripts for analyzing the data are naturally part of the same repository, facilitating reproducibility of obtained results by other researchers.

Next to data, also research software should be FAIR [https://doi.org/10.3233/DS-190026]. WORCS itself sets a good example and follows the recommendations for FAIR research software [https://fair-software.eu/]: It is hosted on a public repository with version control (github), it has a licence (GPL v3.0), it is registered in a community registry (CRAN), it enables the citation of the software (<!-- this paper. anything else? consider adding a cff file -->), and a software quality checklist was used for its development (<!-- which one? -->). By connecting the created R projects to github by default, it also guides its users through the first steps towards making their software FAIR, and lowers the threshold for taking the remaining steps. 




\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
\setlength{\parskip}{0em}

<div id = "refs"></div>
\endgroup
