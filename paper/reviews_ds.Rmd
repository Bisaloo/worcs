---
title       : "Manuscript title"
authors     : "First Author & Second Author"
journal     : "Journal title"
manuscript  : "Manuscript number"

class       : "draft"
bibliography      : ["worcs.bib"]  
output      : papaja::revision_letter_pdf
---

Dear Dr. Editor,

thank you very much for taking the time to consider our manuscript for publication at _`r rmarkdown::metadata$journal`_.
In the following we address your and each reviewers' concerns point-by-point.


# Reviewer \#1

Review #1 submitted on 19/Oct/2020 by Daniel Garijo ORCID logo https://orcid.org/0000-0003-0454-7145

* Overall Impression: Weak
* Suggested Decision: Reject
* Technical Quality of the paper: Good
* Presentation: Good
* Reviewer's confidence: High
* Significance: High significance
* Background: Reasonable
* Novelty: Limited novelty
* Data availability: All used and produced data (if any) are FAIR and openly available in established data repositories
* Length of the manuscript: The length of this manuscript is about right

This paper describes worcs, a package and workflow to help make R code reproducible according to the TOP guidelines.

The paper describes a tutorial of the tool, and how it implements the TOP guidelines.

Reasons to accept:

- The paper is relevant to the journal and on a timely topic.
- The capabilities of the tool for synthetic data generation, connect to OSF, preprint publishing and dependency gathering show a lot of promise.
- All the software (except a 404 page in the help, which is minor) is available online; and the project seems maintained and well documented.

Reasons to reject:

\RC{C1 There is no user evaluation, and therefore all the claims about worcs are unsupported by evidence. If the authors had included a compelling evaluation I would be in favor of accepting the paper.}

Response: We appreciate the recommendation of performing a user evaluation, and this is certainly a next step in the development of worcs. However, as the Editor has indicated, 'resource papers do not need to have a full-blown user evaluation, but we expect "sound evidence of its (potential for) reuse"'. We will thus first address the potential for re-use, and then we will address steps we have taken and intend to take with regard to user evaluation.

First, there are several arguments to substantiate the potential for reuse of worcs:
1) Worcs is implemented in an R-package
2) This R-package is available in a standardized repository, namely CRAN ("The Comprehensive R Archive Network")
3) The software meets all criteria of the Core Infrastructure Initiative "best practices" (CII Best Practices)
4) The R-package uses only tools that are fully integrated within the RStudio IDE, thus enhancing ease of use
5) The R-package is compatible with other reproducibility solutions, including Docker, drake, and make

Second, there is some evidence to demonstrate that worcs is already being adopted (as of 31-10-2020):

1. The preprint has been downloaded 561 times, since being published on 31-05-2020
1. The `worcs` R-package has been downloaded 2401 times from CRAN, since being published on 18-05-2020
1. The GitHub project has been forked by users unknown to us 4 times, and starred (= watched) 32 times
1. The video tutorial for worcs has been viewed 362 times, and liked 18 times (https://www.youtube.com/watch?v=uzjpN_yFeUU)
1. WORCS is used to teach reproducibility at Utrecht University
1. The lead author has given invited lectures on WORCS at the Erasmus University Rotterdam, Eindhoven University of Technology, and Max Planck Institute, Berlin
1. The lead author has applied worcs in several research projects:
	* https://github.com/cjvanlissa/storm_meta_father_mother
	* https://github.com/cjvanlissa/veni_sysrev
	* https://github.com/cjvanlissa/placebo_adhd
	* https://github.com/cjvanlissa/schumpe
	* https://github.com/cjvanlissa/coping_covid


Third, although we have not conducted a user evaluation, there is some evidence that worcs addresses users' needs:
1) Evaluate worcshops!!
2) The issues page on the worcs GitHub profile demonstrates that we have been very responsive in addressing users' feedback (as required by the aforementioned CII Best Practices):
* https://github.com/cjvanlissa/worcs/issues?q=is%3Aissue+is%3Aclosed

<!-- The goal of the present manuscript is to define the problem worcs intends to address (specifically; meeting the TOP-guidelines for Open Science), introduce the tools worcs uses to address this problem, explain how worcs uses these tools, and provide a step-by-step workflow. It is not clear what "claims" are made that are unsupported by evidence; -->

\Assignment{CJ, BV, AL}
\WorkInProgress
\Hard

\RC{C2 This paper describes worcs, a package and workflow to help make R code reproducible according to the TOP guidelines.

The paper is well written, easy to follow and very relevant and timely for the journal. I think the capabilities of the worcs package for synthetic data generation, connect to osf, help with the preprints and capture dependencies are very valuable to the community. However, the main limitation of the paper is that the workflow and proposed package are not evaluated with users, and therefore the claims of the authors with respect of its capabilities are not supported by evidence. I would like to encourage the authors to gather user/community feedback and demonstrate with numbers that worcs can help scientists as they claim. Then, the paper would become a strong contribution for the journal.}

Response: In addressing this comment, we respectfully refer to our response to the preceding comment ("There is no user evaluation", R1), and again emphasize the Editor's comment that "resource papers do not need to have a full-blown user evaluation". We would also like to clarify that the goal of the present manuscript is to define the problem worcs intends to address (specifically; meeting the TOP-guidelines for Open Science), introduce the tools worcs uses to address this problem, explain how worcs uses these tools, and provide a step-by-step workflow. It is not clear what "claims" are made that are unsupported by evidence. In the revision, we have attempted to further clarify this goal from the paper onset: REF. 

We do take the suggestion of performing a user evaluation very seriously, but in our mind, this is a next step. First, the workflow must be published, then it can be taught, and subsequently evaluated.

\Assignment{CJ}
\Done
\Easy

\RC{C3 Unsupported claims (beyond the ones derived from an absence of evaluation): The paper claims in many parts that worcs helps make an entire research project Open and Reproducible. However, at the same time there is also a claim for not supporting replication. Since replication is generally understood as a necessary steps towards reproducibility, I find this kind of contradictory. Maybe the claims can be modified stating that worcs helps documentation and understanding, hence supporting FAIR.}

Response: We understand that our writing comes across as contradictory,
and this apparent contradiction arises from a lack of explicit definitions of the terms "replication" and "reproducibility" in the original manuscript.
We are aware of the oft-lamented lack of consistent definitions for these terms (e.g., Patil, Peng, & Leek, 2019; Plesser, 2018).
In the Revision, we have attempted to address this lack of clarity by including explicit definitions for these terms:

> Throughout this paper, we adhere to the definition of open science as
"the practice of science in such a way that others can collaborate and contribute,
where research data, lab notes and other research processes are freely available,
under terms that enable reuse, redistribution and reproduction of the research and its underlying data and methods" [@vicente-saezOpenScienceNow2018].  
[...] reproducibility can be defined as "re-performing the same analysis with the same code using a different analyst",
and replicability as "re-performing the experiment and collecting new data" [@patilVisualToolDefining2019].

After making these definitions explicit, it becomes clear that the Reviewer's definition of replication as "a necessary steps towards reproducibility" is not consistent with the present paper.

We further wish to clarify that the fact that we address guidelines 1-7, but not guideline 8 ("replication") is consistent with the intended purpose of the TOP-guidelines.
Specifically, the first seven guidelines are intended to ensure the "reproducibility of the reported results based on the originating data" [direct quote from @nosekPromotingOpenResearch2015a]. The eighth criterion, by contrast, is "not formally a transparency standard for authors, [but] addresses journal guidelines for consideration of independent replications for publication" (again a direct quote).
To avoid further confusion or controversy, we now address this distinction between the first 7 guidelines and the final 8th explicitly in the revised manuscript:

> These guidelines describe eight standards for open science,
the first seven of which "account for reproducibility of the reported results
based on the originating data, and for sharing sufficient information to conduct an independent 
replication" [@nosekPromotingOpenResearch2015a].
In this context, 
reproducibility can be defined as "re-performing the same analysis with the same code using a different analyst",
and replicability as "re-performing the experiment and collecting new data" [@patilVisualToolDefining2019].
These guidelines are: 1) Comprehensive
citation of literature, data, materials, and methods; 2) sharing data, 3)
sharing the code required to reproduce analyses, 4) sharing new research
materials, and 5) sharing details of the design and analysis; 6)
pre-registration of studies before data collection, and 7) pre-registration of
the analysis plan prior to analysis.
The eighth criterion is "not formally a transparency standard for authors", but "addresses 
journal guidelines for consideration of independent replications for publication".
WORCS defines the goals of open science in terms of the first seven TOP-guidelines,
and the workflow and its `R` implementation are designed to facilitate compliance therewith.


<!--We wish to fully address this comment, including the criticism of unsupported claims - but it is a bit difficult to do so, given the fundamental misunderstanding about what constitutes "reproducibility". We have stated that worcs helps make an entire research project open and reproducible. It does so by:
1) Providing a conceptual workflow that addresses all steps required to make a project open and reproducible
2) Addressing common bottlenecks, such as the inability to share primary data, and offering solutions to overcome these hurdles
3) Version controlling a project on Git(Hub)
4) Creating a fully reproducible RMarkdown document
5) Managing the package dependencies

The Reviewer, by contrast, proposes that the main contribution of worcs is to help documentation and understanding. It is not immediately clear to us which components of worcs address these two points - except perhaps the readme.md that is created upon initialization of a new project.--> 

\Assignment{CJ, ALL}
\Done
\Hard

\RC{C4: I am confused by the citation "Van Lissa, Caspar J., Aaron Peikert, and Andreas M. Brandmaier. 2020. “Worcs: Workflow for Open Reproducible Code in Science.”". Is this referring to another paper or the actual package? If it's another paper then what is the contribution of this publication?}

Response: We apologize; it appears that there were numerous errors in the parsing of the reference list after rendering the manuscript to HTML - the preferred format of Data Science. The reference is indeed to the R-package. In the revised manuscript, we have repaired the references so that it is clear that this is a software package, et cetera.

\Assignment{CJ}
\WorkInProgress
\Easy


\RC{C5: If the paper is generated as DDG, where are the functions that show it's capabilities? It is also not clear to me that some of the functionalities of Latex would be usable in RMarkdown, such as all the equations.}

Response: We understand that our introduction of the RMarkdown format was a bit too brief, and that readers may wonder whether specific functionalities of LaTeX would be usable in RMarkdown.
To address this comment, the revised manuscript now reads:

> In the `R` implementation of WORCS, we recommend centering a research project
around one dynamically generated `RMarkdown` document [@xieMarkdownDefinitiveGuide2018].
RMarkdown is based on [`Markdown`](https://en.wikipedia.org/wiki/Markdown),
which uses plain-text commands to typeset a document.
It enhances this format with support for R-code, and is compatible with [LaTeX](https://www.latex-project.org/) math typesetting.
For a full overview of the functionality of `RMarkdown`, see @xieMarkdownDefinitiveGuide2018.

\Assignment{CJ}
\Done
\Medium


\RC{C6: It is not clear how worcs addresses depedency management. The section lists a series of tools that can be used, but the section is not very specific to worcs.}

Response: We apologize for the apparent lack of clarity of this section. 
We have restructured and clarified this section to convey the following information:

1) Introduce the general concept of dependency management
2) Introduce `renv`, the solution for dependency management used by the R-implementation of `worcs`
3) Acknowledge the more comprehensive approach to dependency management offered by Docker

We have also, in response to another Reviewer comment, created a Docker Hub build that runs `worcs`. Therefore, as of this revision, both of the tools discussed in this section (i.e., `renv` and Docker) are directly relevant for WORCS-users, and the section should read less like a "series of tools that can be used".
If the Reviewer so desires, we would also be willing to cut the brief discussion of Code Ocean - but this had been included at the request of an informal reviewer of this paper (Daniël Lakens).

\Assignment{CJ}
\Done
\Easy


\RC{C7: The workflow of releasing data products on GitHub is not ideal. First, because datasets of 100 MBs usually require special handling. Second, it doesn't follow very well the FAIR principles for findability and attribution of data. Instead, repositories like Zenodo or FigShare could be user to address data storage, with proper metadata and its corresponding citation. The authors seem to not describe very well what would happen in data science experiments where the data size is significant.}

BV: Can you please argue for why GitHub is equivalent to Zenodo for this purpose?
AL: Can you address the reference to FAIR principles in relation to GitHub?
CJ: I will address "big" data

\Assignment{BV, AB, AL, CJ}
\WorkInProgress
\Hard

\RC{In the 3 phases listed, it is not clear what worcs does and does not for users.}

\Assignment{All}
\WorkInProgress
\Medium


\RC{In some cases it looks like the authors claim that everyone should use R for open science. While R has a strong community behind, I believe there are others with very strong support (e.g. Python). I am not sure if suggesting a shift to R is the right approach. Note that this is the impression I got from reading the paper, maybe it was unintended by the authors.}

Response: Clearly define scope; feedback is welcome.

\Assignment{All}
\WorkInProgress
\Hard


\RC{Vignete on citation leads to a 404.}

\Assignment{CJ}
\WorkInProgress
\Easy


\RC{Synthetic data generation is fantastic, but it is not clear to me how does worcs actually keep the data consistent e.g. for training models. Does it follow a similar distribution to the source data?}

Response: Provide reference, clarify explanation in text.

\Assignment{CJ}
\WorkInProgress
\Easy


\RC{Docker is deemed as complicated for novice users, but the setup vignete looks quite complex to me as well. Wouldn't a Docker container with worcs installed be easier to use? Maybe these aspects would be better highlighted in a user evaluation.}

Response: Thank you for this thoughtful suggestion. We explained better why `renv` is more user friendly (because it builds upon the "normal" package workflow of R users). For users familiar with Docker, we have published an image on Docker Hub with all requirements of worcs installed and a Docker specific setup vignette, that is much shorter and given that you know how to use Docker easier.
\Assignment{AP, AB}
\WorkInProgress
\Medium



# Reviewer \#2
Submitted on 19/Oct/2020 by Remzi Celebi ORCID logo https://orcid.org/0000-0001-7769-4272

* Overall Impression: Good
* Suggested Decision: Accept
* Technical Quality of the paper: Good
* Presentation: Good
* Reviewer's confidence: Medium
* Significance: High significance
* Background: Reasonable
* Novelty: Limited novelty
* Data availability: With exceptions that are admissible according to the data availability guidelines, all used and produced data are FAIR and openly available in established data repositories
* Length of the manuscript: The length of this manuscript is about right

The paper introduces the Workflow for Open Reproducible Code in Science (WORCS) , a workflow that complies with most of best practices for open sciences projects and demonstrates the use of the R worcs package for adapting WORCS workflow. The package provides a template for R users to create projects that follow open science principles.

Reasons to accept:

It provides a good discussion of TOP-guidelines (a set of open science principles) and how proposed workflow/package can be used to facilitate the adaptation of these guidelines. 

Reasons to reject:

\RC{I would suggest the authors include a flowchart for users that shows the decision making process for which tools/operations should be used in WORCS. An example project would also help users better understand the package.}

\Assignment{ALL}
\WorkInProgress
\Medium

\RC{The claim that WORCS is amenable to the FAIR Guiding Principles should be justified. What are the solutions provided by WORCS that make a project interoperable and reusable? It would be improved with a more thorough discussion on how WORCS meets each FAIR principle.}

\Assignment{AL, BV, CJ}
\WorkInProgress
\Medium

\RC{Is WORCS providing any service for de-identification?}

Response: What would this entail? Is it within scope?

\Assignment{ALL}
\WorkInProgress
\Medium

# Reviewer \#3

Submitted on 26/Oct/2020 by Anonymous

* Overall Impression: Weak
* Suggested Decision: Reject
* Technical Quality of the paper: Average
* Presentation: Good
* Reviewer's confidence: High
* Significance: High significance
* Background: Reasonable
* Novelty: Limited novelty
* Data availability: All used and produced data (if any) are FAIR and openly available in established data repositories
* Length of the manuscript: The length of this manuscript is about right

The paper presents WORCS, a library that is implemented within R-studio with the purpose of promoting the creation of reproducible research. In particular, they assist the researchers in the tasks of generating the scholarly article, thereby managing a number of tasks that researchers have to deal with from the reformulation of their hypothesis to version control, to dependency management, and finally generating the scholarly article.

Reasons to accept:

On the positive side, the authors built their tool, with Open Science principles in mind. In doing so, they have pined the actions that need to be performed as part of the research process. For instance, steps such as the pre-registration of the objective of the study before starting the investigation is ignored by existing solutions.

Reasons to reject:

The objective of the author is ambiguous. That said, I do not think that WORCS provide sufficient features that make it stands out from state of the art tools, for the following reasons.

\RC{C1) The process as described by the author for conducting research is assumed to be linear. However, researchers often go back and forth on steps that they have already completed, and sometimes have to undo them completely and start from scratch. This does not seem to be tackled by the tool.}

Response: We agree with the Reviewer that researchers often go back and forth in their work. We did not intend to convey the impression that the process of conducting research is assumed to be linear. Of course, since we are presenting a workflow - there are a number of steps involved, which we have placed in roughly chronological order and numbered for reference purposes. For example, Study Design must logically occur prior to Writing and Analysis, and Publication can only logically occur after Writing. We also wish to emphasize that the "tool" (we assume the Reviewer is referring to the `worcs` R-package) does not enforce any kind of linearity. 

To address the Reviewer's comment, we have made two changes: First, we now explicitly state, in the paper and the Workflow vignette, that the steps are not necessarily linear:  *"Note that, although the steps are numbered for reference purposes, we acknowledge that the process of conducting research is not always linear."*. Second, while it was always possible to manually add a manuscript or preregistration at a later point in time after creating a new project in RStudio - we have now added functions to increase the non-linear flexibility of the `worcs` package: `add_manuscript()` and `add_preregistration()`.

\Assignment{CJ}
\Done
\Easy

\RC{C2) The gist of the tool seems to allow researchers to document their steps and generate a scholarly document that is ready for submission. In a sense, there is a similarity with existing notebooks such as Jupyter. So I was wondering why don’t the authors start from an existing popular tool such as Jupyter as the starting point (given the similarity in features that they would like to add), and extend Jupyter for instance with the new features such as dependency documentation and version control ?}

Response: There are two points we wish to address here. First, we would like to clarify that this paper introduces WORCS, a conceptual workflow for open science research projects. When the Reviewer refers to "the tool" - we assume this to mean the `worcs` R-package, which offers convenience functions to help R-users follow the WORCS-workflow. This is clarified in the Abstract and in the paper, e.g.:

> Abstract: *This paper introduces the Workflow for Open Reproducible Code in Science (WORCS): A step-by-step procedure that researchers can follow to make a research project open and reproducible, accompanied by an open source software implementation that automates these steps.*

> Manuscript, paragraph titled **The R implementation of WORCS**: *WORCS is, first and foremost, a conceptual workflow that could be implemented in any software environment. As of this writing, the workflow has been implemented for `R` users in the package `worcs` [@worcspackage2020].*

Second, in response to the main comment, we agree with the Reviewer that the conceptual workflow as explained in the paper could be implemented in any software environment, including Jupyter. In fact, we emphasize this in the paragraph on *Future developments*: *"although the workflow is currently implemented only in `R`, it is conceptually relevant for users of other statistical programming languages, most notably `Python` [@10.5555/1593511]. We welcome efforts to implement WORCS in other platforms."*.

To answer the Reviewer's question directly: We implemented this workflow in R because we are R developers. But in the paper, we also provide some arguments that compel our preference for R and RStudio as a choice for statistical computing in open science:

> First, `R` and all of it extensions are free open source software, which
make it a tool of choice for open science.
Second, all tools required for an open science workflow are implemented in `R`,
most of these tools are directly accessible through the RStudio user interface,
and some of them are actively developed by the team behind RStudio.
Third, unlike any competitor, RStudio is a Public Benefit Corporation, whose primary purpose is explicitly aligned with open science principles:  
*RStudio’s primary purpose is to create free and open-source software for data science, scientific research, and technical communication. This allows anyone with access to a computer to participate freely in a global economy that rewards data literacy; enhances the production and consumption of knowledge; and facilitates collaboration and reproducible research in science, education and industry.*  
Fourth, `R` is the second-most cited statistical software package [@muenchenPopularityDataScience2012], following SPSS, which has no support for any of the open science tools discussed in this paper.
Fifth, `R` is well-supported by
a vibrant and inclusive online community,
which develops new methods and packages,
and provides support and tutorials for existing ones.
Finally, `R` is highly interoperable:
Packages are available to read and write nearly every filetype,
including DOCX, PDF (in APA style), and HTML. 
Moreover, wrappers are available for many tools developed in other programming languages, and code written in other programming languages can be evaluated from `R`, including `C++`, 
`Fortran`, and `Python` [@allaireMarkdownPythonEngine2020]. There are excellent free
resources for learning `R` [e.g., @grolemundDataScience2017].

\Assignment{BV, CJ}
\Done
\Medium

\RC{C3) Regarding Version control and dependency documentation, the authors uses in their system existing solution, namely git and env, respectively. While I did not criticize reusing existing solution, on the contrary, I don't believe that simply using such solution add a substantial value. For example, git capture changes at the level of the line of code/text, whereas a researcher needs an abstraction that informs him/her on the coarse-grained elements that are significant from the point of view of development and research steps so as to have a clear idea of where s/he is within the process and help him/her make the decision of the next steps to undertake.}

Response: Respectfully, we are not entirely sure what the criticism is here. First, as addressed in our response to the previous comment, there seems to be a misunderstanding about the scope of the paper. To clarify, we again state that our paper introduces a workflow for open science, based on objective criteria set out in the TOP-guidelines. Regarding novelty: Aside from the work in progress of our co-authors - which is focused on strict computational reproducibility and is compatible with the present workflow - this is one of the first efforts to translate the requirements of open science into a step-by-step workflow. When you search Google Scholar for "open science workflow", the first hit that actually describes a workflow is the present manuscript.

Second, the Reviewer claims that using e.g. Git does not add substantial value - but this claim is in disagreement with published literature that makes a strong case for the value of using Git for research [see two references cited in the paper, @ramGitCanFacilitate2013;  @blischakQuickIntroductionVersion2016].

Third, the Reviewer remarks that 
"git capture changes at the level of the line of code/text, whereas a researcher needs an abstraction that informs him/her on the coarse-grained elements [...] so as to have a clear idea of where s/he is within the process and help him/her make the decision of the next steps to undertake".
This comment suggests a fundamental misunderstanding of how Git works.
Although Git tracks file changes on a line-by-line basis,
any number of such changes can be committed together, thereby capturing the more "course-grained elements".
For example, in our response to these reviews, we have created one Git commit for each Reviewer comment, and these commits contain changes to the action letter, manuscript, and vignettes.

We do see the value in clarifying this point in the manuscript as well, and to address this comment, the manuscript now reads:

> Git tracks changes to files on a line-by-line basis. The user can sture these changes by making a "commit": a snapshot of the version controlled files. Each "commit" can contain as many changes as desired; for example, a whole new paragraph, or many small changes made to address a single reviewer comment.

\Assignment{CJ}
\Done
\Medium

\RC{C4: To sum up, I think that the initial idea is interesting. However, I do not think that WORCS will be adopted by researchers or promote the state of the art of reproducibility, even if we focus on researchers who conduct their development and analysis using R.}

Response: We thank the Reviewer for acknowledging the idea to be interesting.
Whether it will be adopted by researchers remains to be seen after publication.
There is some prelimiary evidence that the workflow is being adopted (as of 31-10-2020):

1. The preprint has been downloaded 561 times, since being published on 31-05-2020
1. The `worcs` R-package has been downloaded 2401 times from CRAN, since being published on 18-05-2020
1. The GitHub project has been forked by users unknown to us 4 times, and starred (= watched) 32 times
1. The video tutorial for worcs has been viewed 362 times, and liked 18 times (https://www.youtube.com/watch?v=uzjpN_yFeUU)
1. WORCS is used to teach reproducibility at Utrecht University
1. The lead author has given invited lectures on WORCS at the Erasmus University Rotterdam, Eindhoven University of Technology, and Max Planck Institute, Berlin
1. The lead author has applied worcs in several research projects:
	* https://github.com/cjvanlissa/storm_meta_father_mother
	* https://github.com/cjvanlissa/veni_sysrev
	* https://github.com/cjvanlissa/placebo_adhd
	* https://github.com/cjvanlissa/schumpe
	* https://github.com/cjvanlissa/coping_covid

\Assignment{CJ}
\Done
\Easy

# Reviewer \#4

Submitted on 26/Oct/2020 by Katherine Wolstencroft https://orcid.org/0000-0002-1279-5133

* Overall Impression: Good
* Suggested Decision: Accept
* Technical Quality of the paper: Excellent
* Presentation: Good
* Reviewer's confidence: Medium
* Significance: Moderate significance
* Background: Reasonable
* Novelty: Limited novelty
* Data availability: All used and produced data (if any) are FAIR and openly available in established data repositories
* Length of the manuscript: The length of this manuscript is about right

This paper describes a Workflow for Open Reproducible Code in Science (WORCS). It promotes reproducible and FAIR R code and complies with the TOP-guidelines. The workflow is presented with an R library and a Github repository containing worcs templates and all supporting materials for this manuscript.

Reasons to accept:

The paper is well written and clearly describes WORCS, which promotes reproducible and open code for scientific investigations. It complies with current best practices for open and reproducible code and it also follows the FAIR principles. Such initiatives help promote better open and reproducible science. The approach taken here is pragmatic and lightweight, which is necessary for large scale uptake. The authors also focus on ease of use and provide a "one-click solution".

WORCS is for scientists working with R with RMarkdown. It could therefore be argued that the utility is limited. However, R is widely used across science and the approach taken here could serve as an example for other software.

Reasons to reject:

\RC{C1: The abstract of the paper states that the manuscript "provides examples of the implementation of worcs in R", so I was expecting science projects that had been described using worcs. If these examples exist, they should be referenced and linked to from the manuscript, as they would provide a better demonstration of the practical utility of the workflow and software than tutorial style examples.}

Response: We thank the Reviewer for this helpful suggestion! Although such user examples are not yet numerous, the lead author has six public WORCS repositories, and there are two public repositories by other users. In our opinion, it would be most useful to have a continuously cumulating list of example repositories, and link that list in the paper - rather than to publish a static list. Therefore, to address this comment, we have scripted a web scraper that searches GitHub for WORCS repositories (using the metadata tags created by the `worcs` package). We have embedded the scraper in the README.Rmd file on the worcs GitHub page, which is regularly updated. In the revised paper, we point readers to [this readme](https://github.com/cjvanlissa/worcs) for a list of user examples, in the Abstract, and when introducing the workflow.

\Assignment{CJ}
\Done
\Hard

\RC{C2: There is a comparison in the paper to another R-based reproducible science solution (Peikert and Brandmaier (2019)). The authors discuss some of the differences between these approaches, but the comparison is limited. It is not clear to me how different these approaches actually are, apart from ease of use. If scientists are already familiar with R, is there a significant learning curve with either solution?}

We agree that this is an important point, and have rewritten the paragraph comparing these two solutions (as cited below). We hope that this adresses the Reviewer's question. However, as the workflow of Peikert and Brandmaier is still in the preprint stage, and the software implementation has not yet been developed - a more detailed comparison of these two approaches would be premature.

> A different class of solutions instead focuses on the practical issue of *how*
researchers can meet the requirements of open science.
A notable example is the workflow for reproducible analyses developed by @peikertReproducibleDataAnalysis2019,
which uses Docker to ensure strict computational reproducibility for
even the most sophisticated analyses.
There are some key differences with WORCS.
First, with regard to the scope of the workflow: 
WORCS is designed to address a unique issue not covered by other existing solutions,
namely to provide a workflow most conducive to satisfying TOP-guidelines 1-7 while adhering to the FAIR principles.
Peikert and Brandmaier instead focus primarily on computational reproducibility,
which is relevant for TOP-guidelines 2, 3, and 5.
Second, with regard to ease of use,
WORCS aims to bring down the learning curve by adopting sensible defaults for any decisions to be made.
Peikert and Brandmaier, by contrast, designed their workflow to be very flexible and comprehensive,
thus requiring substantially more background knowledge and technical sophistication from users.
To sum up, WORCS builds upon the same general principles
as Peikert and Brandmaier, and the two workflows are compatible. What 
sets WORCS apart is that it is a more lightweight
approach to safeguard computational reproducibility under *most circumstances*, 
which is easier to adopt for projects that can be conducted entirely within `R`.


\Assignment{CJ, AP, AB}
\WorkInProgress
\Medium

# Editor

Submitted by Tobias Kuhn on Tue, 10/27/2020 - 02:04, http://orcid.org/0000-0002-1267-0234

\RC{The reviewers agree that the paper has merit, but some of them also point to major shortcomings, in particular the lack of discussion of user adoption and of handling non-linear research processes. Note that according to our guidelines, resource papers do not need to have a full-blown user evaluation, but we expect "sound evidence of its (potential for) reuse".}

Response: We thank the Editor and all Reviewers for their helpful comments on this manuscript. We have attempted to address all of these points in the action letter above. With regard to the specific point reiterated by the Editor:

### User adoption

We thank the Editor for pointing out that a full-blown user evaluation is not required for a resource paper.
Of course, we take this Reviewer suggestion very seriously, and intend to conduct a user evaluation after the paper is published and users begin to adopt the workflow.
In the meantime, we present the following evidence for WORCS' potential for reuse:

TODO

### Non-linear research processes

Although Reviewer 3 claims that WORCS assumes a linear research process, this is incorrect.

We agree with Reviewer 3 that researchers often go back and forth in their work. 
We did not intend to convey the impression that the process of conducting research is assumed to be linear. 
Of course, since we are presenting a workflow - there are a number of steps involved, which we have placed in roughly chronological order and numbered for reference purposes.
For example, Study Design must logically occur prior to Writing and Analysis, and Publication can only logically occur after Writing.
We also wish to emphasize that the "tool" (we assume the Reviewer is referring to the `worcs` R-package) does not enforce any kind of linearity. 

To address the Reviewer's comment, we have made two changes: First, we now explicitly state, in the paper and the Workflow vignette, that the steps are not necessarily linear:  *"Note that, although the steps are numbered for reference purposes, we acknowledge that the process of conducting research is not always linear."*. Second, while it was always possible to manually add a manuscript or preregistration at a later point in time after creating a new project in RStudio - we have now added functions to make non-linear workflows even more convenient, specifically: `add_manuscript()` and `add_preregistration()`.

\Assignment{AP, AB}
\WorkInProgress
\Medium

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
